{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis** \n",
    "\n",
    "This model will classify the text into positive or negative (sometimes neutral) sentiments in its most basic form. \n",
    "\n",
    "Naturally, the most successful approaches are using supervised models that need a fair amount of labelled data to be trained. However, providing such data is an expensive and time-consuming process that is not possible or readily accessible in many cases.\n",
    "\n",
    "\n",
    "The output of such models is a number implying how similar the text is to the positive examples we provided during the training and does not conside nuances sucha s sentiment complexity of the text.\n",
    "\n",
    "\n",
    "This is a unsupervised semantic model that captures the overall sentiment of the text and, at the same time, provides a way to analyze the polarity strength and complexity of emotions while maintaining high performance. In a sense, the distance between each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processing and Data manipulation\n",
    "import numpy as np # linear algenra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries and packages for NLP\n",
    "import nltk\n",
    "# It includes a set of text \n",
    "# processing libraries for classification, tokenization, \n",
    "# stemming, tagging, parsing, and semantic reasonin\n",
    "import gensim\n",
    "# library for unsupervised topic modeling, \n",
    "# document indexing, retrieval by similarity, and \n",
    "# other natural language processing functionalities, \n",
    "# using modern statistical machine learning.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "      <td>today senate dems vote to savetheinternet prou...</td>\n",
       "      <td>today senate dem vote savetheinternet proud su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "      <td>nbclatino noted that hurricane maria has left ...</td>\n",
       "      <td>nbclatino note hurricane maria leave approxima...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Party         Handle   \n",
       "0           0  Democrat  RepDarrenSoto  \\\n",
       "1           1  Democrat  RepDarrenSoto   \n",
       "2           2  Democrat  RepDarrenSoto   \n",
       "\n",
       "                                               Tweet   \n",
       "0  Today, Senate Dems vote to #SaveTheInternet. P...  \\\n",
       "1  RT @WinterHavenSun: Winter Haven resident / Al...   \n",
       "2  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...   \n",
       "\n",
       "                                             cleaned   \n",
       "0  today senate dems vote to savetheinternet prou...  \\\n",
       "1  winterhavensun winter haven resident alta vist...   \n",
       "2  nbclatino noted that hurricane maria has left ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  today senate dem vote savetheinternet proud su...  \n",
       "1  winterhavensun winter haven resident alta vist...  \n",
       "2  nbclatino note hurricane maria leave approxima...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path =\"C:\\\\Users\\\\CACER\\\\OneDrive\\\\Desktop\\\\lemmatized_tweets.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding `src` directory to the directories for interpreter to search\n",
    "sys.path.append(os.path.abspath(os.path.join('../..','w2v_utils.py')))\n",
    "\n",
    "\n",
    "# Importing functions and classes from utility module\n",
    "from w2v_utils import (Tokenizer,\n",
    "                       w2v_trainer,\n",
    "                       calculate_overall_similarity_score,\n",
    "                       overall_semantic_sentiment_analysis\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancing the Tokenizer class\n",
    "tokenizer = Tokenizer(clean= True,\n",
    "                      lower= True, \n",
    "                      de_noise= True, \n",
    "                      remove_stop_words= True,\n",
    "                      keep_negation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import w2v_utils\n",
    "importlib.reload(w2v_utils)\n",
    "from w2v_utils import(Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.511700e+04\n",
       "mean             -inf\n",
       "std               NaN\n",
       "min              -inf\n",
       "25%      2.079442e+00\n",
       "50%      2.302585e+00\n",
       "75%      2.397895e+00\n",
       "max      3.044522e+00\n",
       "Name: tokenized_vectors_len, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize reviews\n",
    "df['tokenized_vectors'] = df['lemmatized'].apply(tokenizer.tokenize)\n",
    "\n",
    "df['tokenized_vectors_len'] = df['tokenized_vectors'].apply(len)\n",
    "df['tokenized_vectors_len'].apply(np.log).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised Approach**\n",
    "***Semantic Similarity Approach [SSA]***\n",
    "\n",
    "First, train a word embedding model using all the reviews. Next, I will choose two sets of words that hold positive and negative sentiments expressed commonly in the movie review context. Then, to predict the sentiment of a review, we will calculate the text's similarity in the word embedding space to these positive and negative sets and see which sentiment the text is closest to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the word embedding model**\n",
    "The approach we will be doing is called word2vec as the model converts words into vectors in an embedding space. Since we don't need to split our dataset into and test for building unsupervised models. We will train the model on the all thw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized_vectors</th>\n",
       "      <th>tokenized_vectors_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "      <td>today senate dems vote to savetheinternet prou...</td>\n",
       "      <td>today senate dem vote savetheinternet proud su...</td>\n",
       "      <td>[today, senate, dem, vote, savetheinternet, pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "      <td>[winterhavensun, winter, resident, alta, vista...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "      <td>nbclatino noted that hurricane maria has left ...</td>\n",
       "      <td>nbclatino note hurricane maria leave approxima...</td>\n",
       "      <td>[nbclati, NOnote, hurricane, maria, leave, app...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Party         Handle   \n",
       "0           0  Democrat  RepDarrenSoto  \\\n",
       "1           1  Democrat  RepDarrenSoto   \n",
       "2           2  Democrat  RepDarrenSoto   \n",
       "\n",
       "                                               Tweet   \n",
       "0  Today, Senate Dems vote to #SaveTheInternet. P...  \\\n",
       "1  RT @WinterHavenSun: Winter Haven resident / Al...   \n",
       "2  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...   \n",
       "\n",
       "                                             cleaned   \n",
       "0  today senate dems vote to savetheinternet prou...  \\\n",
       "1  winterhavensun winter haven resident alta vist...   \n",
       "2  nbclatino noted that hurricane maria has left ...   \n",
       "\n",
       "                                          lemmatized   \n",
       "0  today senate dem vote savetheinternet proud su...  \\\n",
       "1  winterhavensun winter haven resident alta vist...   \n",
       "2  nbclatino note hurricane maria leave approxima...   \n",
       "\n",
       "                                   tokenized_vectors  tokenized_vectors_len  \n",
       "0  [today, senate, dem, vote, savetheinternet, pr...                     10  \n",
       "1  [winterhavensun, winter, resident, alta, vista...                     10  \n",
       "2  [nbclati, NOnote, hurricane, maria, leave, app...                     10  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSUPERVISED LEARNING MODEL FOR TWEETS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the CARE, FAIRNESS, INGROUP, AUTHORITY, and PURITY sets** \n",
    "\n",
    "There is no unique formula to choose the moral foundtion sets becuase each morality has a postive and negative. However, we checked the most similar words to the words 'care', 'fairnes', 'ingroup', 'authority', and 'purity' in our newly trained embedding space to have a starting point. Mixing it with my judgment on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Word2Vec model\n",
    "keyed_vectors, keyed_vocab = w2v_trainer(df['tokenized_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOTchoose', 0.7063878774642944),\n",
       " ('betterway', 0.7019863724708557),\n",
       " ('vulnerable', 0.7013588547706604),\n",
       " ('contraception', 0.699955403804779),\n",
       " ('insurance', 0.6969007253646851),\n",
       " ('jeopardize', 0.6907920837402344),\n",
       " ('momsdontneed', 0.687316358089447),\n",
       " ('children', 0.685024619102478),\n",
       " ('insurer', 0.6753472089767456),\n",
       " ('preventative', 0.6737899780273438),\n",
       " ('doctor', 0.6676217913627625),\n",
       " ('prioritize', 0.6664213538169861),\n",
       " ('healthcare', 0.6652873754501343),\n",
       " ('option', 0.6587548851966858),\n",
       " ('mental', 0.6573622822761536)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"care/harm\" \n",
    "keyed_vectors.most_similar(positive=['care','harm'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_harm_concepts = ['care', 'benefit', 'amity','caring','compassion', 'empath', 'guard', 'peace', 'protect', 'safe', 'secure', 'shelter', 'shield', 'sympathy', 'abuse', 'annihilate', 'attack', 'brutal', 'cruelty', 'crush', 'damage', 'destroy', 'detriment', 'endanger', 'fight', 'harm', 'hurt', 'kill'] \n",
    "care_concepts = [concept for concept in care_harm_concepts if concept in keyed_vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theft', 0.8212836980819702),\n",
       " ('identity', 0.8194223642349243),\n",
       " ('disclosure', 0.7705772519111633),\n",
       " ('prohibit', 0.7666760087013245),\n",
       " ('openness', 0.7645556926727295),\n",
       " ('prohibition', 0.7632705569267273),\n",
       " ('pregnan', 0.758402943611145),\n",
       " ('penalty', 0.7514911890029907),\n",
       " ('lending', 0.7490752339363098),\n",
       " ('elimination', 0.7468199729919434),\n",
       " ('accountability', 0.7431881427764893),\n",
       " ('regulate', 0.7364729046821594),\n",
       " ('aim', 0.733492374420166),\n",
       " ('regulator', 0.7334159016609192),\n",
       " ('payday', 0.7321057319641113)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"Fairness/cheating\" \n",
    "keyed_vectors.most_similar(positive=['fairness','fraud'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_cheat_concepts = ['fair', 'balance', 'constant','egalitarian','equable', 'equal', 'equity', 'fairminded', 'honest', 'fairly', 'impartial', 'justice', 'tolerant', 'bias', 'bigotry', 'discrimination', 'dishonest', 'exclusion', 'favoritism', 'inequitable', 'injustice', 'preference', 'prejudice', 'segregation', 'unequal', 'unfair', 'unjust'] \n",
    "fair_concepts = [concept for concept in fair_cheat_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disagreeable', 0.8356388211250305),\n",
       " ('insensitive', 0.8330069184303284),\n",
       " ('erratic', 0.8296713829040527),\n",
       " ('deuteronomy', 0.8289493322372437),\n",
       " ('deliberately', 0.8289366364479065),\n",
       " ('visceral', 0.8284211158752441),\n",
       " ('enabling', 0.8269577622413635),\n",
       " ('noticeably', 0.8210905194282532),\n",
       " ('indifference', 0.8206204175949097),\n",
       " ('NOThate', 0.8167920708656311),\n",
       " ('demeanor', 0.8166794776916504),\n",
       " ('assert', 0.8157222270965576),\n",
       " ('gamble', 0.813547670841217),\n",
       " ('cronyism', 0.8135278224945068),\n",
       " ('distortion', 0.8132206797599792)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"loyalty/betrayal\" \n",
    "keyed_vectors.most_similar(positive=['loyalty','betrayal'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loyal_betrayal_concepts = ['ally', 'cadre', 'clique','cohort','collective', 'communal', 'community', 'comrade', 'devote', 'familial', 'families', 'family', 'fellow', 'group', 'deceive', 'enemy', 'foregin', 'immigrant', 'imposter', 'individual', 'jilt', 'miscreant', 'renegade', 'sequester', 'spy', 'terrorist'] \n",
    "loyal_concepts = [concept for concept in loyal_betrayal_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potentially', 0.8559315204620361),\n",
       " ('abhorrent', 0.845737874507904),\n",
       " ('islamicstate', 0.8407920002937317),\n",
       " ('objectively', 0.8365526795387268),\n",
       " ('inspection', 0.8332947492599487),\n",
       " ('toxic', 0.832619309425354),\n",
       " ('flagrant', 0.8300837874412537),\n",
       " ('deterrent', 0.829119086265564),\n",
       " ('instinct', 0.8267803192138672),\n",
       " ('improper', 0.8257858753204346),\n",
       " ('heinous', 0.8252667784690857),\n",
       " ('restriction', 0.8230447769165039),\n",
       " ('handling', 0.8218958377838135),\n",
       " ('identify', 0.8195672035217285),\n",
       " ('chemical', 0.8168391585350037)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"Authority/Subversion\" \n",
    "keyed_vectors.most_similar(positive=['authority','destruction'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_sub_concepts = ['abide', 'allegiance', 'authority','class','command', 'compliant', 'control', 'defer', 'father', 'hierarchy', 'duty', 'honor', 'law', 'leader', 'agitate', 'alienate', 'defector', 'defiant', 'defy', 'denounce', 'disobey', 'disrespect', 'dissent', 'dissident', 'illegal', 'insubordinate', 'insurgent', 'obstruct'] \n",
    "auth_concepts = [concept for concept in auth_sub_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yep', 0.846430242061615),\n",
       " ('surrounding', 0.8462924361228943),\n",
       " ('evolve', 0.8391093015670776),\n",
       " ('willingness', 0.8368761539459229),\n",
       " ('venality', 0.8366503715515137),\n",
       " ('islamophobia', 0.8359322547912598),\n",
       " ('savethecensus', 0.8342856168746948),\n",
       " ('avivezra', 0.83255535364151),\n",
       " ('stoke', 0.831658661365509),\n",
       " ('coherent', 0.8297153115272522),\n",
       " ('rival', 0.8288025856018066),\n",
       " ('turpitude', 0.8284118175506592),\n",
       " ('soviet', 0.8283827304840088),\n",
       " ('homegrown', 0.8267050981521606),\n",
       " ('qu', 0.824522852897644)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"sanctity/degradation\" \n",
    "keyed_vectors.most_similar(positive=['innocence','degradation'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_degrad_concepts = ['austerity', 'celibate', 'chaste','church','clean', 'decent', 'holy', 'immaculate', 'innocent', 'modest', 'pious', 'pristine', 'pure', 'sacred', 'adultery', 'blemish', 'contagious', 'debase', 'debauchery', 'defile', 'desecrate', 'dirt', 'disease', 'disgust', 'exploitation', 'filth', 'gross', 'impiety'] \n",
    "san_concepts = [concept for concept in san_degrad_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tyranny', 0.8074524998664856),\n",
       " ('alive', 0.7909101247787476),\n",
       " ('coushould', 0.7854657769203186),\n",
       " ('nazi', 0.7833741903305054),\n",
       " ('shall', 0.7804561257362366),\n",
       " ('perish', 0.778619647026062),\n",
       " ('atrocity', 0.7784929275512695),\n",
       " ('religious', 0.7727745175361633),\n",
       " ('determined', 0.7721160054206848),\n",
       " ('ideal', 0.7705406546592712),\n",
       " ('religion', 0.7687275409698486),\n",
       " ('persecution', 0.7677063345909119),\n",
       " ('vow', 0.7634913325309753),\n",
       " ('slavery', 0.7484354376792908),\n",
       " ('intolerance', 0.7456870675086975)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"liberty/oppression\" \n",
    "keyed_vectors.most_similar(positive=['liberty','oppression'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_opp_concepts = ['blameless', 'canon', 'character','commendable','correct', 'decent', 'doctrine', 'ethics', 'exemplary', 'good', 'goodness', 'honest', 'legal', 'integrity', 'bad', 'evil', 'immoral', 'indecent', 'offend', 'offensive', 'transgress', 'wicked', 'wretched', 'wrong'] \n",
    "lib_concepts = [concept for concept in lib_opp_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Semantic Sentiment Scores by OSSA model\n",
    "overall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n",
    "                                                   care_target_tokens= care_concepts, \n",
    "                                                   fair_target_tokens= fair_concepts,\n",
    "                                                   loyal_target_tokens= loyal_concepts,\n",
    "                                                   auth_target_tokens= auth_concepts,\n",
    "                                                   san_target_tokens= san_concepts,\n",
    "                                                   lib_target_tokens= lib_concepts,\n",
    "                                                   doc_tokens = df['tokenized_vectors'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating the semantic sentiment of the reviews**\n",
    "\n",
    "We will be calculating the similarity to the negative and positive sets. For future reference the similarities negative semantic score will be NSS and positive semantic scores will be PSS respectively.We will build the document vector by averaging over the wordvectors building it. In that way, we will have a vector for every review and two vectors representing our positive and negative sets. The PSS and NSS can then be calculated by a simple cosine similarity between the review vector and the positive and negative vectors respectively. This approach will be called  Overall Semantic Sentiment Analysis (OSSA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store semantic sentiment store computed by OSSA model in df\n",
    "df['overall_care'] = overall_df_scores[0] \n",
    "df['overall_fair'] = overall_df_scores[1] \n",
    "df['overall_loyal'] = overall_df_scores[2]\n",
    "df['overall_auth'] = overall_df_scores[3]\n",
    "df['overall_san'] = overall_df_scores[4]\n",
    "df['overall_lib'] = overall_df_scores[5]\n",
    "df['overall_max_score'] = overall_df_scores[6]\n",
    "df['moral_foundations'] = overall_df_scores[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized_vectors</th>\n",
       "      <th>tokenized_vectors_len</th>\n",
       "      <th>overall_care</th>\n",
       "      <th>overall_fair</th>\n",
       "      <th>overall_loyal</th>\n",
       "      <th>overall_auth</th>\n",
       "      <th>overall_san</th>\n",
       "      <th>overall_lib</th>\n",
       "      <th>overall_max_score</th>\n",
       "      <th>moral_foundations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "      <td>today senate dems vote to savetheinternet prou...</td>\n",
       "      <td>today senate dem vote savetheinternet proud su...</td>\n",
       "      <td>[today, senate, dem, vote, savetheinternet, pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.247883</td>\n",
       "      <td>0.425635</td>\n",
       "      <td>0.225297</td>\n",
       "      <td>0.353416</td>\n",
       "      <td>0.244540</td>\n",
       "      <td>0.320193</td>\n",
       "      <td>0.425635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "      <td>winterhavensun winter haven resident alta vist...</td>\n",
       "      <td>[winterhavensun, winter, resident, alta, vista...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.300397</td>\n",
       "      <td>0.267038</td>\n",
       "      <td>0.414613</td>\n",
       "      <td>0.384081</td>\n",
       "      <td>0.390339</td>\n",
       "      <td>0.157723</td>\n",
       "      <td>0.414613</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "      <td>nbclatino noted that hurricane maria has left ...</td>\n",
       "      <td>nbclatino note hurricane maria leave approxima...</td>\n",
       "      <td>[nbclati, NOnote, hurricane, maria, leave, app...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.580558</td>\n",
       "      <td>0.387241</td>\n",
       "      <td>0.441899</td>\n",
       "      <td>0.307524</td>\n",
       "      <td>0.568917</td>\n",
       "      <td>0.479782</td>\n",
       "      <td>0.580558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NALCABPolicy: Meeting with @RepDarrenSoto ...</td>\n",
       "      <td>nalcabpolicy meeting with thanks for taking th...</td>\n",
       "      <td>nalcabpolicy meeting thank take time meet lati...</td>\n",
       "      <td>[nalcabpolicy, meeting, thank, take, time, mee...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.035545</td>\n",
       "      <td>-0.003813</td>\n",
       "      <td>0.154244</td>\n",
       "      <td>0.253586</td>\n",
       "      <td>0.114772</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.253586</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @Vegalteno: Hurricane season starts on June...</td>\n",
       "      <td>vegalteno hurricane season starts on june st p...</td>\n",
       "      <td>vegalteno hurricane season start june st puert...</td>\n",
       "      <td>[vegalte, NOhurricane, season, start, june, st...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.459377</td>\n",
       "      <td>0.330249</td>\n",
       "      <td>0.319653</td>\n",
       "      <td>0.227322</td>\n",
       "      <td>0.609098</td>\n",
       "      <td>0.293931</td>\n",
       "      <td>0.609098</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Party         Handle   \n",
       "0           0  Democrat  RepDarrenSoto  \\\n",
       "1           1  Democrat  RepDarrenSoto   \n",
       "2           2  Democrat  RepDarrenSoto   \n",
       "3           3  Democrat  RepDarrenSoto   \n",
       "4           4  Democrat  RepDarrenSoto   \n",
       "\n",
       "                                               Tweet   \n",
       "0  Today, Senate Dems vote to #SaveTheInternet. P...  \\\n",
       "1  RT @WinterHavenSun: Winter Haven resident / Al...   \n",
       "2  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...   \n",
       "3  RT @NALCABPolicy: Meeting with @RepDarrenSoto ...   \n",
       "4  RT @Vegalteno: Hurricane season starts on June...   \n",
       "\n",
       "                                             cleaned   \n",
       "0  today senate dems vote to savetheinternet prou...  \\\n",
       "1  winterhavensun winter haven resident alta vist...   \n",
       "2  nbclatino noted that hurricane maria has left ...   \n",
       "3  nalcabpolicy meeting with thanks for taking th...   \n",
       "4  vegalteno hurricane season starts on june st p...   \n",
       "\n",
       "                                          lemmatized   \n",
       "0  today senate dem vote savetheinternet proud su...  \\\n",
       "1  winterhavensun winter haven resident alta vist...   \n",
       "2  nbclatino note hurricane maria leave approxima...   \n",
       "3  nalcabpolicy meeting thank take time meet lati...   \n",
       "4  vegalteno hurricane season start june st puert...   \n",
       "\n",
       "                                   tokenized_vectors  tokenized_vectors_len   \n",
       "0  [today, senate, dem, vote, savetheinternet, pr...                     10  \\\n",
       "1  [winterhavensun, winter, resident, alta, vista...                     10   \n",
       "2  [nbclati, NOnote, hurricane, maria, leave, app...                     10   \n",
       "3  [nalcabpolicy, meeting, thank, take, time, mee...                     11   \n",
       "4  [vegalte, NOhurricane, season, start, june, st...                     12   \n",
       "\n",
       "   overall_care  overall_fair  overall_loyal  overall_auth  overall_san   \n",
       "0      0.247883      0.425635       0.225297      0.353416     0.244540  \\\n",
       "1      0.300397      0.267038       0.414613      0.384081     0.390339   \n",
       "2      0.580558      0.387241       0.441899      0.307524     0.568917   \n",
       "3      0.035545     -0.003813       0.154244      0.253586     0.114772   \n",
       "4      0.459377      0.330249       0.319653      0.227322     0.609098   \n",
       "\n",
       "   overall_lib  overall_max_score  moral_foundations  \n",
       "0     0.320193           0.425635                  1  \n",
       "1     0.157723           0.414613                  2  \n",
       "2     0.479782           0.580558                  0  \n",
       "3    -0.000684           0.253586                  3  \n",
       "4     0.293931           0.609098                  4  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
