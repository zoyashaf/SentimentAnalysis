{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e14-fbFkvh47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "import spacy  # For preprocessing\n",
        "import numpy as np\n",
        "from langdetect import detect\n",
        "import swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>bucket</th>\n",
              "      <th>annotator</th>\n",
              "      <th>annotation</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator01</td>\n",
              "      <td>Purity</td>\n",
              "      <td>Confident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Confident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator00</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text subreddit  \\\n",
              "0  That particular part of the debate is especial...    europe   \n",
              "1  That particular part of the debate is especial...    europe   \n",
              "2  That particular part of the debate is especial...    europe   \n",
              "3  /r/france is pretty lively, with it's own ling...    europe   \n",
              "4  /r/france is pretty lively, with it's own ling...    europe   \n",
              "\n",
              "            bucket    annotator     annotation          confidence  \n",
              "0  French politics  annotator03      Non-Moral           Confident  \n",
              "1  French politics  annotator01         Purity           Confident  \n",
              "2  French politics  annotator02  Thin Morality           Confident  \n",
              "3  French politics  annotator03      Non-Moral           Confident  \n",
              "4  French politics  annotator00      Non-Moral  Somewhat Confident  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = '/Users/shafz/OneDrive/Documents/deep-learning-final-project-yelp_reviews_classification/data/raw'\n",
        "tweets = pd.read_csv(path+'/reddit_raw.csv')\n",
        "tweets = tweets.drop('Unnamed: 0', axis = 1)\n",
        "tweets.head(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EHc2dbhW0K4e"
      },
      "source": [
        "# Data Cleaning \n",
        "Lets take a look at some reviews to see what data cleaning measures we need \n",
        "* We can see that we need to remove punctuation and that some reviews have a lot of white space \n",
        "* Both uppercase and lowercase letters are present \n",
        "* There are numerical digits as well \n",
        "* Some reviews contain expressions such as \"this/that\" and simply removing '/' results in \"thisthat\" instead of \"this that\" \n",
        "* Some reviews contained only punctuation (e.g. '.', ': )', '*') \n",
        "* Some reviews do not have proper spacing (e.g. 'Don't misinterpret my review....I' which turns into 'dont misinterpret my reviewi')\n",
        "* There are reviews in Chinese and Spanish \n",
        "* Had some strange letters show up such as 'entrÃ©es' due to encoding/decoding issues\n",
        "* Some reviews have website links in them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNCN4mmV0KRi",
        "outputId": "a0255304-f2a1-4030-b7f0-2726510e6d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review 1:\n",
            " That particular part of the debate is especially funny. Macron was explaining he did not think FN voters were evil and that from where he comes from he knew many, and she was like \"ooooh the evil FN invaders they're everywhere...!\" Self-awareness: zero.\n",
            "Review 2:\n",
            " That particular part of the debate is especially funny. Macron was explaining he did not think FN voters were evil and that from where he comes from he knew many, and she was like \"ooooh the evil FN invaders they're everywhere...!\" Self-awareness: zero.\n",
            "Review 3:\n",
            " That particular part of the debate is especially funny. Macron was explaining he did not think FN voters were evil and that from where he comes from he knew many, and she was like \"ooooh the evil FN invaders they're everywhere...!\" Self-awareness: zero.\n",
            "Review 4:\n",
            " /r/france is pretty lively, with it's own lingo that is usually deliberately bad translations - shitpost is \"cacapoteau\" or \"compost\", crosspost is \"croixpoteau\", etc. There's a mix of memes, politics, international news, personal problems, etc.\n",
            "\n",
            "The election time was pretty entertaining, with people from /r/the_doofus coming over avd shilling for Le Pen, usually very badly.\n",
            "Review 5:\n",
            " /r/france is pretty lively, with it's own lingo that is usually deliberately bad translations - shitpost is \"cacapoteau\" or \"compost\", crosspost is \"croixpoteau\", etc. There's a mix of memes, politics, international news, personal problems, etc.\n",
            "\n",
            "The election time was pretty entertaining, with people from /r/the_doofus coming over avd shilling for Le Pen, usually very badly.\n",
            "Review 1:\n",
            " Well I can discern from your vehemence toward a woman wanting what she thinks may be equality to her husband that you have problems with relationships.  I feel bad for any woman in your life that wants more than to not starve.\n",
            "Review 2:\n",
            " Kick! Punch! It's all in the mind. If you wanna test me, I'm sure you'll find. The things I'll teach ya is sure to beat ya. But nevertheless you'll get a lesson from teacher.\n",
            "Review 3:\n",
            " Reddit can’t help you this is some seriously traumatic shit ... he should move to the coast fuck\n",
            "\n",
            "\n",
            "Or she sorry\n",
            "Review 4:\n",
            " Yes. Disordered eating is insidious. And Rita needs help, and not the sort that involves the OP changing anything.\n",
            "Review 5:\n",
            " What parent would let a kid bring a Gameboy to church?\n"
          ]
        }
      ],
      "source": [
        "for index,text in enumerate(tweets['text'][:5]):\n",
        "  print('Review %d:\\n'%(index+1),text)\n",
        "for index,text in enumerate(tweets['text'][-5:]):\n",
        "  print('Review %d:\\n'%(index+1),text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vCO-4CBnAWlg"
      },
      "outputs": [],
      "source": [
        "## Expanding Contractions \n",
        "contractions_dict = { \"ain't\": \"are not \",\"'s\":\" is \",\"aren't\": \"are not \",\n",
        "                     \"can't\": \"cannot \",\"can't've\": \"cannot have \",\n",
        "                     \"'cause\": \"because \",\"could've\": \"could have \",\"couldn't\": \"could not \",\n",
        "                     \"didn't\": \"did not \",\"doesn't\": \"does not \",\n",
        "                     \"don't\": \"do not \",\"hadn't\": \"had not \",\n",
        "                     \"hasn't\": \"has not \",\"haven't\": \"have not \",\"he'd\": \"he would \",\n",
        "                     \"he'd've\": \"he would have \",\"he'll \": \"he will \",\n",
        "                     \"how'd\": \"how did \",\"how'll\": \"how will \",\n",
        "                     \"I'd\": \"I would \", \"I'll \": \"I will \",\n",
        "                     \"I'm\": \"I am \",\"I've\": \"I have \", \"isn't\": \"is not \",\n",
        "                     \"it'd\": \"it would \",\"it'll\": \"it will \",\n",
        "                     \"let's\": \"let us \",\"ma'am\": \"madam \",\n",
        "                     \"might've\": \"might have \",\"mightn't\": \"might not \", \n",
        "                     \"must've\": \"must have \",\"mustn't\": \"must not \",\n",
        "                     \"needn't\": \"need not \", \"shan't\": \"shall not\" ,\n",
        "                     \"she'd\": \"she would \",\n",
        "                     \"she'll\": \"she will \", \"should've\": \"should have \",\n",
        "                     \"shouldn't\": \"should not \", \n",
        "                     \"that'd\": \"that would \",  \"there'd\": \"there would \",\n",
        "                     \"they'd\": \"they would \",\n",
        "                     \"they'll\": \"they will \",\n",
        "                     \"they're\": \"they are \",\"they've\": \"they have \",\n",
        "                     \"to've\": \"to have \",\"wasn't\": \"was not \",\"we'd\": \"we would \",\n",
        "                     \"we'll\": \"we will \",\n",
        "                     \"we're\": \"we are \",\"we've\": \"we have \", \"weren't\": \"were not \",\"what'll\": \"what will \",\n",
        "                     \"what're\": \"what are \", \"what've\": \"what have \",\n",
        "                     \"when've\": \"when have \",\"where'd\": \"where did \", \"where've\": \"where have \",\n",
        "                     \"who'll\": \"who will \",\"who've\": \"who have \",\n",
        "                     \"why've\": \"why have \",\"will've\": \"will have \",\"won't\": \"will not \",\n",
        "                     \"would've\": \"would have \",\"wouldn't\": \"would not \",\n",
        "                     \"y'all\": \"you all \", \"you'd\": \"you would \",\n",
        "                     \"you'll\": \"you will \", \"you're\": \"you are \",\n",
        "                     \"you've\": \"you have \"}\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rWvyJ5_dAg0s"
      },
      "outputs": [],
      "source": [
        "# expanding contractions \n",
        "tweets['cleaned'] = tweets['text'].apply(lambda x:expand_contractions(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "61221    Well I can discern from your vehemence toward ...\n",
              "61222    Kick! Punch! It is  all in the mind. If you wa...\n",
              "61223    Reddit can’t help you this is some seriously t...\n",
              "61224    Yes. Disordered eating is insidious. And Rita ...\n",
              "61225    What parent would let a kid bring a Gameboy to...\n",
              "Name: cleaned, dtype: object"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets['cleaned'][-5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3C9Y0Zcp0_70"
      },
      "outputs": [],
      "source": [
        "## Making all words lowercase, removing punctuation, URLs, and white spaces \n",
        "tweets['cleaned'] = tweets['cleaned'].apply(lambda x: x.replace('/r/', '').strip())\n",
        "tweets['cleaned']  = tweets['cleaned'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', ' ', x))\n",
        "tweets['cleaned']  = tweets['cleaned'].str.lower().apply(lambda x: re.sub(r\"[\\d\\n\\-\\./]+\", ' ', x))\n",
        "tweets['cleaned']  = tweets['cleaned'].apply(lambda x: re.sub(' +',' ',x))\n",
        "tweets['cleaned']  = tweets['cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "tweets['cleaned'] = tweets['cleaned'].apply(lambda x: x.replace('op ', '').strip())\n",
        "tweets['cleaned'] = tweets['cleaned'].apply(lambda x: x.replace('gt ', '').strip())\n",
        "tweets['cleaned']  = tweets['cleaned'].replace(' ', np.nan)\n",
        "tweets['cleaned']  = tweets['cleaned'].replace('', np.nan)\n",
        "# tweets['cleaned'] = tweets['cleaned'].apply(lambda x: re.sub(' +',' ',x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>bucket</th>\n",
              "      <th>annotator</th>\n",
              "      <th>annotation</th>\n",
              "      <th>confidence</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>61221</th>\n",
              "      <td>Well I can discern from your vehemence toward ...</td>\n",
              "      <td>AmItheAsshole</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Equality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>well i can discern from your vehemence toward ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61222</th>\n",
              "      <td>Kick! Punch! It's all in the mind. If you wann...</td>\n",
              "      <td>nostalgia</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>kick punch it is all in the mind if you wanna ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61223</th>\n",
              "      <td>Reddit can’t help you this is some seriously t...</td>\n",
              "      <td>confession</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>reddit cant help you this is some seriously tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61224</th>\n",
              "      <td>Yes. Disordered eating is insidious. And Rita ...</td>\n",
              "      <td>AmItheAsshole</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>yes disordered eating is insidious and rita ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61225</th>\n",
              "      <td>What parent would let a kid bring a Gameboy to...</td>\n",
              "      <td>nostalgia</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Authority</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>what parent would let a kid bring a gameboy to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text      subreddit  \\\n",
              "61221  Well I can discern from your vehemence toward ...  AmItheAsshole   \n",
              "61222  Kick! Punch! It's all in the mind. If you wann...      nostalgia   \n",
              "61223  Reddit can’t help you this is some seriously t...     confession   \n",
              "61224  Yes. Disordered eating is insidious. And Rita ...  AmItheAsshole   \n",
              "61225  What parent would let a kid bring a Gameboy to...      nostalgia   \n",
              "\n",
              "                  bucket    annotator     annotation          confidence  \\\n",
              "61221  Everyday Morality  annotator05       Equality           Confident   \n",
              "61222  Everyday Morality  annotator05  Thin Morality  Somewhat Confident   \n",
              "61223  Everyday Morality  annotator05  Thin Morality           Confident   \n",
              "61224  Everyday Morality  annotator05      Non-Moral  Somewhat Confident   \n",
              "61225  Everyday Morality  annotator05      Authority  Somewhat Confident   \n",
              "\n",
              "                                                 cleaned  \n",
              "61221  well i can discern from your vehemence toward ...  \n",
              "61222  kick punch it is all in the mind if you wanna ...  \n",
              "61223  reddit cant help you this is some seriously tr...  \n",
              "61224  yes disordered eating is insidious and rita ne...  \n",
              "61225  what parent would let a kid bring a gameboy to...  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text           0\n",
              "subreddit      0\n",
              "bucket         0\n",
              "annotator      0\n",
              "annotation     0\n",
              "confidence    42\n",
              "cleaned        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKBX9oY8ClA9",
        "outputId": "5a8d9334-51cd-41c4-89a8-b75422af1d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review 1:\n",
            " that particular part of the debate is especially funny macron was explaining he did not think fn voters were evil and that from where he comes from he knew many and she was like ooooh the evil fn invaders they are everywhere  self awareness zero\n",
            "Review 2:\n",
            " that particular part of the debate is especially funny macron was explaining he did not think fn voters were evil and that from where he comes from he knew many and she was like ooooh the evil fn invaders they are everywhere  self awareness zero\n",
            "Review 3:\n",
            " that particular part of the debate is especially funny macron was explaining he did not think fn voters were evil and that from where he comes from he knew many and she was like ooooh the evil fn invaders they are everywhere  self awareness zero\n",
            "Review 4:\n",
            " france is pretty lively with it is own lingo that is usually deliberately bad translations shitpost is cacapoteau or compost crosspost is croixpoteau etc there is a mix of memes politics international news personal problems etc the election time was pretty entertaining with people from the_doofus coming over avd shilling for le pen usually very badly\n",
            "Review 5:\n",
            " france is pretty lively with it is own lingo that is usually deliberately bad translations shitpost is cacapoteau or compost crosspost is croixpoteau etc there is a mix of memes politics international news personal problems etc the election time was pretty entertaining with people from the_doofus coming over avd shilling for le pen usually very badly\n",
            "Review 1:\n",
            " well i can discern from your vehemence toward a woman wanting what she thinks may be equality to her husband that you have problems with relationships i feel bad for any woman in your life that wants more than to not starve\n",
            "Review 2:\n",
            " kick punch it is all in the mind if you wanna test me i am sure you will find the things i will teach ya is sure to beat ya but nevertheless you will get a lesson from teacher\n",
            "Review 3:\n",
            " reddit cant help you this is some seriously traumatic shit he should move to the coast fuck or she sorry\n",
            "Review 4:\n",
            " yes disordered eating is insidious and rita needs help and not the sort that involves the changing anything\n",
            "Review 5:\n",
            " what parent would let a kid bring a gameboy to church\n"
          ]
        }
      ],
      "source": [
        "for index,text in enumerate(tweets['cleaned'][:5]):\n",
        "  print('Review %d:\\n'%(index+1),text)\n",
        "for index,text in enumerate(tweets['cleaned'][-5:]):\n",
        "  print('Review %d:\\n'%(index+1),text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "n3I9KaOZhs1C"
      },
      "outputs": [],
      "source": [
        "# path = '/Users/shafz/OneDrive/Documents/deep-learning-final-project-yelp_reviews_classification/data/interim'\n",
        "# tweets.to_csv(path + './og_reddit_cleaned.csv', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "slo6BGrvDRR2"
      },
      "source": [
        "Removing stop words, lemmatizing, and tokenizing \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = '/Users/shafz/OneDrive/Documents/deep-learning-final-project-yelp_reviews_classification/data/interim'\n",
        "tweets = pd.read_csv(path + './og_reddit_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "yR0f7ABSDVbV"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xel_-rrHhQg",
        "outputId": "861bf6c7-809f-4b78-dad1-76710a5fd1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'please', 'we', 'some', 'say', 'thereupon', 'hereby', 'never', 'toward', 'ever', 'nine', 'know', 'want', 'same', 'hence', '’ll', 'where', 'us', 'our', 'might', 'all', 'elsewhere', 'moreover', 'before', 'you', 'out', 'except', 'anywhere', 'either', 'thence', 'forty', 'under', 'until', 'side', 'she', 'must', 'becomes', '’s', 'nowhere', 'whereby', 'being', 'and', 'call', 'he', 'think', 'whereas', 'if', 'go', 'every', 'herein', 'is', 'sixty', 'sometime', 'by', 'yours', 'already', 'upon', 'his', 'mine', 'various', 'nor', 'on', \"'ve\", 'although', 'show', 'whither', 'still', 'yourselves', 'hereupon', 'wherever', 'people', 'then', 'bottom', 'several', '‘re', 'across', 'ours', 'however', 'your', 'see', 'not', 'latter', 'am', 'around', 'least', 'six', 'twenty', 'empty', 'because', 'after', 'here', 'take', 'why', 'becoming', 'ourselves', 'than', 'noone', 'further', 'anything', 'how', 'between', 'latterly', 'or', 'unless', 'put', 'them', 'former', 'four', 'of', 'another', 'onto', 'seems', 'have', 'himself', 'over', 'other', 'whose', 'alone', 'but', 'when', 'used', 'him', 'behind', 'three', 'who', 'with', 'are', 'back', 'within', 'last', 'which', 'well', 'anyhow', 'down', 'namely', 'should', 'was', 'keep', 'besides', 'its', 'whether', 'ten', 'nothing', 'doing', 'from', 'these', 'like', 'into', 'one', 'whenever', 'became', 'twelve', 'there', 'really', 'beyond', 'often', 'seeming', 'almost', 'through', 'does', 'the', 'themselves', 'this', 'anyway', 'each', 'part', 'therefore', 'hereafter', 'tell', 'itself', 'among', 'third', 'more', 'has', 'their', 'just', 'to', 'may', 'due', 'below', 'no', 'though', 'thing', 'serious', \"'re\", 'done', 'for', 'less', 'regarding', 'everything', 'my', '‘d', 'fifteen', 'amount', 'cannot', 'such', 'n‘t', 'along', 'five', 'quite', 'it', 'anyone', 'full', 'also', 'become', 'above', 'nevertheless', 'make', '’re', 'do', 'thereby', 'whoever', 'about', 'so', 'myself', 'come', 'at', 'afterwards', 'neither', '‘s', 'would', 'others', 'most', 'towards', 'been', 'during', 'name', 'they', 'while', 're', 'whole', 'somewhere', 'up', '’d', '‘m', \"'d\", 'enough', 'everywhere', 'seem', 'somehow', 'both', 'top', 'mostly', 'again', \"'ll\", 'thereafter', 'against', 'had', 'per', 'someone', 'fifty', 'whatever', 'none', 'without', 'her', 'once', 'off', 'get', 'give', 'indeed', 'own', 'therein', 'thru', 'front', 'beside', 'eleven', 'else', 'that', 'formerly', 'hundred', 'very', 'yourself', 'perhaps', 'next', 'amongst', 'too', 'herself', 'much', 'eight', 'an', 'many', 'first', '‘ve', 'everyone', 'could', 'did', 'me', 'even', 'a', 'made', \"n't\", 'using', 'seemed', 'since', 'ca', '’m', 'nobody', 'n’t', 'hers', 'only', 'will', \"'m\", \"'s\", 'wherein', 'those', 'whom', 'now', 'via', 'something', 'any', 'be', '’ve', 'yet', '‘ll', 'throughout', 'together', 'meanwhile', 'two', 'as', 'whereafter', 'in', 'need', 'were', 'sometimes', 'beforehand', 'whereupon', 'thus', 'move', 'whence', 'few', 'can', 'what', 'rather', 'i', 'otherwise'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(nlp.Defaults.stop_words)\n",
        "len(nlp.Defaults.stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.Defaults.stop_words -= {'not', 'always'}\n",
        "nlp.Defaults.stop_words |= {'go', 'come', 'not', 'like', 'thing', 'people', 'think', 'want', 'tell', 'want', 'need', 'know'}\n",
        "len(nlp.Defaults.stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "aw1DvjyWeNLs"
      },
      "outputs": [],
      "source": [
        "texts = tweets['cleaned'].tolist()\n",
        "lemmatized_texts = []\n",
        "for doc in nlp.pipe(texts, batch_size=1000, n_process=4):\n",
        "    lemmatized_texts.append(' '.join([token.lemma_ for token in doc if (token.is_stop==False)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets['lemmatized'] = lemmatized_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text           0\n",
              "subreddit      0\n",
              "bucket         0\n",
              "annotator      0\n",
              "annotation     0\n",
              "confidence    42\n",
              "cleaned        0\n",
              "lemmatized     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>bucket</th>\n",
              "      <th>annotator</th>\n",
              "      <th>annotation</th>\n",
              "      <th>confidence</th>\n",
              "      <th>cleaned</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>that particular part of the debate is especial...</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator01</td>\n",
              "      <td>Purity</td>\n",
              "      <td>Confident</td>\n",
              "      <td>that particular part of the debate is especial...</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>that particular part of the debate is especial...</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>france is pretty lively with it is own lingo t...</td>\n",
              "      <td>france pretty lively lingo usually deliberatel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator00</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>france is pretty lively with it is own lingo t...</td>\n",
              "      <td>france pretty lively lingo usually deliberatel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>france is pretty lively with it is own lingo t...</td>\n",
              "      <td>france pretty lively lingo usually deliberatel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TBH Marion Le Pen would be better. Closet fasc...</td>\n",
              "      <td>neoliberal</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>tbh marion le pen would be better closet fasci...</td>\n",
              "      <td>tbh marion le pen well closet fascist vs flamb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>TBH Marion Le Pen would be better. Closet fasc...</td>\n",
              "      <td>neoliberal</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator00</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Not Confident</td>\n",
              "      <td>tbh marion le pen would be better closet fasci...</td>\n",
              "      <td>tbh marion le pen well closet fascist vs flamb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TBH Marion Le Pen would be better. Closet fasc...</td>\n",
              "      <td>neoliberal</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Equality</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>tbh marion le pen would be better closet fasci...</td>\n",
              "      <td>tbh marion le pen well closet fascist vs flamb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>it really is a very unusual situation isn't it...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>it really is a very unusual situation is not i...</td>\n",
              "      <td>unusual situation fillon affair influence vote...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   subreddit  \\\n",
              "0  That particular part of the debate is especial...      europe   \n",
              "1  That particular part of the debate is especial...      europe   \n",
              "2  That particular part of the debate is especial...      europe   \n",
              "3  /r/france is pretty lively, with it's own ling...      europe   \n",
              "4  /r/france is pretty lively, with it's own ling...      europe   \n",
              "5  /r/france is pretty lively, with it's own ling...      europe   \n",
              "6  TBH Marion Le Pen would be better. Closet fasc...  neoliberal   \n",
              "7  TBH Marion Le Pen would be better. Closet fasc...  neoliberal   \n",
              "8  TBH Marion Le Pen would be better. Closet fasc...  neoliberal   \n",
              "9  it really is a very unusual situation isn't it...      europe   \n",
              "\n",
              "            bucket    annotator     annotation          confidence  \\\n",
              "0  French politics  annotator03      Non-Moral           Confident   \n",
              "1  French politics  annotator01         Purity           Confident   \n",
              "2  French politics  annotator02  Thin Morality           Confident   \n",
              "3  French politics  annotator03      Non-Moral           Confident   \n",
              "4  French politics  annotator00      Non-Moral  Somewhat Confident   \n",
              "5  French politics  annotator02      Non-Moral           Confident   \n",
              "6  French politics  annotator03      Non-Moral  Somewhat Confident   \n",
              "7  French politics  annotator00  Thin Morality       Not Confident   \n",
              "8  French politics  annotator02       Equality  Somewhat Confident   \n",
              "9  French politics  annotator03      Non-Moral           Confident   \n",
              "\n",
              "                                             cleaned  \\\n",
              "0  that particular part of the debate is especial...   \n",
              "1  that particular part of the debate is especial...   \n",
              "2  that particular part of the debate is especial...   \n",
              "3  france is pretty lively with it is own lingo t...   \n",
              "4  france is pretty lively with it is own lingo t...   \n",
              "5  france is pretty lively with it is own lingo t...   \n",
              "6  tbh marion le pen would be better closet fasci...   \n",
              "7  tbh marion le pen would be better closet fasci...   \n",
              "8  tbh marion le pen would be better closet fasci...   \n",
              "9  it really is a very unusual situation is not i...   \n",
              "\n",
              "                                          lemmatized  \n",
              "0  particular debate especially funny macron expl...  \n",
              "1  particular debate especially funny macron expl...  \n",
              "2  particular debate especially funny macron expl...  \n",
              "3  france pretty lively lingo usually deliberatel...  \n",
              "4  france pretty lively lingo usually deliberatel...  \n",
              "5  france pretty lively lingo usually deliberatel...  \n",
              "6  tbh marion le pen well closet fascist vs flamb...  \n",
              "7  tbh marion le pen well closet fascist vs flamb...  \n",
              "8  tbh marion le pen well closet fascist vs flamb...  \n",
              "9  unusual situation fillon affair influence vote...  "
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets.drop('cleaned', axis = 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>bucket</th>\n",
              "      <th>annotator</th>\n",
              "      <th>annotation</th>\n",
              "      <th>confidence</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator01</td>\n",
              "      <td>Purity</td>\n",
              "      <td>Confident</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>particular debate especially funny macron expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Confident</td>\n",
              "      <td>france pretty lively lingo usually deliberatel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator00</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>france pretty lively lingo usually deliberatel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61221</th>\n",
              "      <td>Well I can discern from your vehemence toward ...</td>\n",
              "      <td>AmItheAsshole</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Equality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>discern vehemence woman want think equality hu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61222</th>\n",
              "      <td>Kick! Punch! It's all in the mind. If you wann...</td>\n",
              "      <td>nostalgia</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>kick punch mind wanna test sure find thing tea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61223</th>\n",
              "      <td>Reddit can’t help you this is some seriously t...</td>\n",
              "      <td>confession</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Thin Morality</td>\n",
              "      <td>Confident</td>\n",
              "      <td>reddit not help seriously traumatic shit coast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61224</th>\n",
              "      <td>Yes. Disordered eating is insidious. And Rita ...</td>\n",
              "      <td>AmItheAsshole</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Non-Moral</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>yes disorder eat insidious rita need help sort...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61225</th>\n",
              "      <td>What parent would let a kid bring a Gameboy to...</td>\n",
              "      <td>nostalgia</td>\n",
              "      <td>Everyday Morality</td>\n",
              "      <td>annotator05</td>\n",
              "      <td>Authority</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>parent let kid bring gameboy church</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61226 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text      subreddit  \\\n",
              "0      That particular part of the debate is especial...         europe   \n",
              "1      That particular part of the debate is especial...         europe   \n",
              "2      That particular part of the debate is especial...         europe   \n",
              "3      /r/france is pretty lively, with it's own ling...         europe   \n",
              "4      /r/france is pretty lively, with it's own ling...         europe   \n",
              "...                                                  ...            ...   \n",
              "61221  Well I can discern from your vehemence toward ...  AmItheAsshole   \n",
              "61222  Kick! Punch! It's all in the mind. If you wann...      nostalgia   \n",
              "61223  Reddit can’t help you this is some seriously t...     confession   \n",
              "61224  Yes. Disordered eating is insidious. And Rita ...  AmItheAsshole   \n",
              "61225  What parent would let a kid bring a Gameboy to...      nostalgia   \n",
              "\n",
              "                  bucket    annotator     annotation          confidence  \\\n",
              "0        French politics  annotator03      Non-Moral           Confident   \n",
              "1        French politics  annotator01         Purity           Confident   \n",
              "2        French politics  annotator02  Thin Morality           Confident   \n",
              "3        French politics  annotator03      Non-Moral           Confident   \n",
              "4        French politics  annotator00      Non-Moral  Somewhat Confident   \n",
              "...                  ...          ...            ...                 ...   \n",
              "61221  Everyday Morality  annotator05       Equality           Confident   \n",
              "61222  Everyday Morality  annotator05  Thin Morality  Somewhat Confident   \n",
              "61223  Everyday Morality  annotator05  Thin Morality           Confident   \n",
              "61224  Everyday Morality  annotator05      Non-Moral  Somewhat Confident   \n",
              "61225  Everyday Morality  annotator05      Authority  Somewhat Confident   \n",
              "\n",
              "                                              lemmatized  \n",
              "0      particular debate especially funny macron expl...  \n",
              "1      particular debate especially funny macron expl...  \n",
              "2      particular debate especially funny macron expl...  \n",
              "3      france pretty lively lingo usually deliberatel...  \n",
              "4      france pretty lively lingo usually deliberatel...  \n",
              "...                                                  ...  \n",
              "61221  discern vehemence woman want think equality hu...  \n",
              "61222  kick punch mind wanna test sure find thing tea...  \n",
              "61223  reddit not help seriously traumatic shit coast...  \n",
              "61224  yes disorder eat insidious rita need help sort...  \n",
              "61225                parent let kid bring gameboy church  \n",
              "\n",
              "[61226 rows x 7 columns]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = '/Users/shafz/OneDrive/Documents/deep-learning-final-project-yelp_reviews_classification/data/processed/'\n",
        "tweets.to_csv(path + 'lemmatized_reddit_og.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
