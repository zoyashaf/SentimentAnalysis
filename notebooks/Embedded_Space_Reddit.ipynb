{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gdown\\cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vT1g4Crj6s6z2YNjSyPx5Fvz5KTpBYSB\n",
      "To: c:\\Users\\CACER\\OneDrive\\Desktop\\deep-learning-final-project-yelp_reviews_classification\\notebooks\\annottated_reddit.csv\n",
      "\n",
      "  0%|          | 0.00/1.51M [00:00<?, ?B/s]\n",
      " 35%|███▍      | 524k/1.51M [00:00<00:00, 4.40MB/s]\n",
      "100%|██████████| 1.51M/1.51M [00:00<00:00, 7.69MB/s]\n",
      "C:\\Users\\CACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gdown\\cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1N0U-i4PxT_DmTru0r7T5po-9f9cwGgB8\n",
      "To: c:\\Users\\CACER\\OneDrive\\Desktop\\deep-learning-final-project-yelp_reviews_classification\\notebooks\\RedditComments2.csv\n",
      "\n",
      "  0%|          | 0.00/4.30M [00:00<?, ?B/s]\n",
      " 12%|█▏        | 524k/4.30M [00:00<00:00, 4.00MB/s]\n",
      " 49%|████▉     | 2.10M/4.30M [00:00<00:00, 8.96MB/s]\n",
      " 85%|████████▌ | 3.67M/4.30M [00:00<00:00, 11.5MB/s]\n",
      "100%|██████████| 4.30M/4.30M [00:00<00:00, 10.3MB/s]\n",
      "C:\\Users\\CACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gdown\\cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dlV7huXQfim8brL7LIBy-M2kFR63P2Oh\n",
      "To: c:\\Users\\CACER\\OneDrive\\Desktop\\deep-learning-final-project-yelp_reviews_classification\\notebooks\\Reddit_metadata2.csv\n",
      "\n",
      "  0%|          | 0.00/15.3k [00:00<?, ?B/s]\n",
      "100%|██████████| 15.3k/15.3k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1vT1g4Crj6s6z2YNjSyPx5Fvz5KTpBYSB -O annottated_reddit.csv\n",
    "!gdown --id 1N0U-i4PxT_DmTru0r7T5po-9f9cwGgB8 -O RedditComments2.csv\n",
    "!gdown --id 1dlV7huXQfim8brL7LIBy-M2kFR63P2Oh -O Reddit_metadata2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processing and Data manipulation\n",
    "import numpy as np # linear algenra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries and packages for NLP\n",
    "import nltk\n",
    "# It includes a set of text \n",
    "# processing libraries for classification, tokenization, \n",
    "# stemming, tagging, parsing, and semantic reasonin\n",
    "import gensim\n",
    "# library for unsupervised topic modeling, \n",
    "# document indexing, retrieval by similarity, and \n",
    "# other natural language processing functionalities, \n",
    "# using modern statistical machine learning.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"annottated_reddit.csv\")\n",
    "df.drop_duplicates(inplace= True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>bucket</th>\n",
       "      <th>annotator</th>\n",
       "      <th>annotation</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP doesn't need to wait for a referendum to b...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator00</td>\n",
       "      <td>Authority</td>\n",
       "      <td>Not Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or - or - assclowns like Le Pen and Farage cou...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator02</td>\n",
       "      <td>Equality</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations on your victory Macron voters....</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator00</td>\n",
       "      <td>Care</td>\n",
       "      <td>Not Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The German Constitution did not let Hitler bec...</td>\n",
       "      <td>neoliberal</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator02</td>\n",
       "      <td>Equality</td>\n",
       "      <td>Somewhat Confident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So Republicans really are for liberal policies...</td>\n",
       "      <td>politics</td>\n",
       "      <td>US Politics</td>\n",
       "      <td>annotator03</td>\n",
       "      <td>Equality</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   subreddit   \n",
       "0  MLP doesn't need to wait for a referendum to b...      europe  \\\n",
       "1  Or - or - assclowns like Le Pen and Farage cou...   worldnews   \n",
       "2  Congratulations on your victory Macron voters....   worldnews   \n",
       "3  The German Constitution did not let Hitler bec...  neoliberal   \n",
       "4  So Republicans really are for liberal policies...    politics   \n",
       "\n",
       "            bucket    annotator annotation          confidence  \n",
       "0  French politics  annotator00  Authority       Not Confident  \n",
       "1  French politics  annotator02   Equality           Confident  \n",
       "2  French politics  annotator00       Care       Not Confident  \n",
       "3  French politics  annotator02   Equality  Somewhat Confident  \n",
       "4      US Politics  annotator03   Equality           Confident  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding `src` directory to the directories for interpreter to search\n",
    "sys.path.append(os.path.abspath(os.path.join('../..','w2v_utils.py')))\n",
    "\n",
    "\n",
    "# Importing functions and classes from utility module\n",
    "from w2v_utils import (Tokenizer,\n",
    "                       w2v_trainer,\n",
    "                       calculate_overall_similarity_score,\n",
    "                       overall_semantic_sentiment_analysis\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancing the Tokenizer class\n",
    "tokenizer = Tokenizer(clean= True,\n",
    "                      lower= True, \n",
    "                      de_noise= True, \n",
    "                      remove_stop_words= True,\n",
    "                      keep_negation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['subreddit', 'bucket', 'annotator', 'confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP doesn't need to wait for a referendum to b...</td>\n",
       "      <td>Authority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or - or - assclowns like Le Pen and Farage cou...</td>\n",
       "      <td>Equality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations on your victory Macron voters....</td>\n",
       "      <td>Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The German Constitution did not let Hitler bec...</td>\n",
       "      <td>Equality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So Republicans really are for liberal policies...</td>\n",
       "      <td>Equality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text annotation\n",
       "0  MLP doesn't need to wait for a referendum to b...  Authority\n",
       "1  Or - or - assclowns like Le Pen and Farage cou...   Equality\n",
       "2  Congratulations on your victory Macron voters....       Care\n",
       "3  The German Constitution did not let Hitler bec...   Equality\n",
       "4  So Republicans really are for liberal policies...   Equality"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annotation\n",
       "Care               973\n",
       "Authority          814\n",
       "Equality           618\n",
       "Proportionality    472\n",
       "Loyalty            306\n",
       "Purity             231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"annotation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping \n",
    "df = df[df[\"annotation\"].str.contains(\"Equality,Proportionality\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Purity,Equality\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Purity,Proportionality\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Loyalty,Equality\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Purity,Loyalty\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Proportionality,Loyalty\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Equality,Authority\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Proportionality,Authority\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Loyalty,Care\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Loyalty,Purity\") == False]\n",
    "df = df[df[\"annotation\"].str.contains(\"Purity,Loyalty,Equality\") == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_values(df, input_column, output_column):\n",
    "    df[output_column] = df[input_column].apply(lambda x: 0 if (x == 'Care') else 1 if (x == 'Equality') else 2 if (x == 'Loyalty') else 3 if (x == 'Authority') else 4 if (x == 'Purity') else 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_values(df, \"annotation\", \"labeled_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>labeled_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP doesn't need to wait for a referendum to b...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or - or - assclowns like Le Pen and Farage cou...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations on your victory Macron voters....</td>\n",
       "      <td>Care</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The German Constitution did not let Hitler bec...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So Republicans really are for liberal policies...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text annotation  labeled_data\n",
       "0  MLP doesn't need to wait for a referendum to b...  Authority             3\n",
       "1  Or - or - assclowns like Le Pen and Farage cou...   Equality             1\n",
       "2  Congratulations on your victory Macron voters....       Care             0\n",
       "3  The German Constitution did not let Hitler bec...   Equality             1\n",
       "4  So Republicans really are for liberal policies...   Equality             1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3414.000000\n",
       "mean        2.799016\n",
       "std         0.610728\n",
       "min         1.098612\n",
       "25%         2.302585\n",
       "50%         2.772589\n",
       "75%         3.258097\n",
       "max         4.442651\n",
       "Name: tokenized_vectors_len, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize reviews\n",
    "df['tokenized_vectors'] = df['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "df['tokenized_vectors_len'] = df['tokenized_vectors'].apply(len)\n",
    "df['tokenized_vectors_len'].apply(np.log).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>labeled_data</th>\n",
       "      <th>tokenized_vectors</th>\n",
       "      <th>tokenized_vectors_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP doesn't need to wait for a referendum to b...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>3</td>\n",
       "      <td>[mlp, NOTneed, wait, referendum, break, europe...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Or - or - assclowns like Le Pen and Farage cou...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>1</td>\n",
       "      <td>[assclowns, like, le, pen, farage, could, demo...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations on your victory Macron voters....</td>\n",
       "      <td>Care</td>\n",
       "      <td>0</td>\n",
       "      <td>[congratulations, victory, macron, voters, kno...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text annotation  labeled_data   \n",
       "0  MLP doesn't need to wait for a referendum to b...  Authority             3  \\\n",
       "1  Or - or - assclowns like Le Pen and Farage cou...   Equality             1   \n",
       "2  Congratulations on your victory Macron voters....       Care             0   \n",
       "\n",
       "                                   tokenized_vectors  tokenized_vectors_len  \n",
       "0  [mlp, NOTneed, wait, referendum, break, europe...                     53  \n",
       "1  [assclowns, like, le, pen, farage, could, demo...                     22  \n",
       "2  [congratulations, victory, macron, voters, kno...                     36  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Word2Vec model\n",
    "keyed_vectors, keyed_vocab = w2v_trainer(df['tokenized_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('children', 0.9999149441719055),\n",
       " ('without', 0.9998925924301147),\n",
       " ('op', 0.9998899102210999),\n",
       " ('sorry', 0.9998860359191895),\n",
       " ('around', 0.9998852610588074),\n",
       " ('respect', 0.9998831748962402),\n",
       " ('nta', 0.9998764395713806),\n",
       " ('behavior', 0.9998753666877747),\n",
       " ('friend', 0.9998745322227478),\n",
       " ('live', 0.9998738765716553),\n",
       " ('man', 0.999873697757721),\n",
       " ('sex', 0.9998733401298523),\n",
       " ('parents', 0.999872088432312),\n",
       " ('others', 0.9998708963394165),\n",
       " ('wife', 0.9998690485954285)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"care/harm\" \n",
    "keyed_vectors.most_similar(positive=['care','harm'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_harm_concepts = ['care', 'benefit', 'amity','caring','compassion', 'empath', 'guard', 'peace', 'protect', 'safe', 'secure', 'shelter', 'shield', 'sympathy', 'abuse', 'annihilate', 'attack', 'brutal', 'cruelty', 'crush', 'damage', 'destroy', 'detriment', 'endanger', 'fight', 'harm', 'hurt', 'kill'] \n",
    "care_concepts = [concept for concept in care_harm_concepts if concept in keyed_vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('made', 0.9999108910560608),\n",
       " ('everything', 0.9999091029167175),\n",
       " ('making', 0.9999021887779236),\n",
       " ('maybe', 0.9998984932899475),\n",
       " ('police', 0.9998982548713684),\n",
       " ('whole', 0.9998943209648132),\n",
       " ('lack', 0.9998937249183655),\n",
       " ('tax', 0.9998927712440491),\n",
       " ('back', 0.9998916983604431),\n",
       " ('usually', 0.9998915195465088),\n",
       " ('rather', 0.9998907446861267),\n",
       " ('wife', 0.9998902678489685),\n",
       " ('old', 0.9998878240585327),\n",
       " ('person', 0.9998875260353088),\n",
       " ('society', 0.9998866319656372)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"Fairness/cheating\" \n",
    "keyed_vectors.most_similar(positive=['fair','cheating'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_cheat_concepts = ['fair', 'balance', 'constant','egalitarian','equable', 'equal', 'equity', 'fairminded', 'honest', 'fairly', 'impartial', 'justice', 'tolerant', 'bias', 'bigotry', 'discrimination', 'dishonest', 'exclusion', 'favoritism', 'inequitable', 'injustice', 'preference', 'prejudice', 'segregation', 'unequal', 'unfair', 'unjust'] \n",
    "fair_concepts = [concept for concept in fair_cheat_concepts if concept in keyed_vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('imagine', 0.9983861446380615),\n",
       " ('violence', 0.998374879360199),\n",
       " ('whole', 0.9983651041984558),\n",
       " ('animals', 0.9983639121055603),\n",
       " ('lot', 0.9983518719673157),\n",
       " ('since', 0.998348593711853),\n",
       " ('usually', 0.9983477592468262),\n",
       " ('anything', 0.9983435273170471),\n",
       " ('feelings', 0.9983433485031128),\n",
       " ('use', 0.9983403086662292),\n",
       " ('part', 0.9983401894569397),\n",
       " ('back', 0.9983378052711487),\n",
       " ('yes', 0.9983372092247009),\n",
       " ('everything', 0.9983369708061218),\n",
       " ('year', 0.9983366131782532)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyed_vectors.most_similar(positive=['loyalty','betrayal'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loyal_betrayal_concepts = ['ally', 'cadre', 'clique','cohort','collective', 'communal', 'community', 'comrade', 'devote', 'familial', 'families', 'family', 'fellow', 'group', 'deceive', 'enemy', 'foregin', 'immigrant', 'imposter', 'individual', 'jilt', 'miscreant', 'renegade', 'sequester', 'spy', 'terrorist'] \n",
    "loyal_concepts = [concept for concept in loyal_betrayal_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 0.9995715618133545),\n",
       " ('part', 0.9995705485343933),\n",
       " ('yes', 0.9995692372322083),\n",
       " ('completely', 0.9995673298835754),\n",
       " ('NOTeven', 0.9995670914649963),\n",
       " ('sure', 0.9995666742324829),\n",
       " ('violence', 0.9995647072792053),\n",
       " ('obviously', 0.9995626211166382),\n",
       " ('given', 0.9995619654655457),\n",
       " ('getting', 0.9995616674423218),\n",
       " ('wrong', 0.9995594024658203),\n",
       " ('cause', 0.99955815076828),\n",
       " ('hell', 0.999557614326477),\n",
       " ('find', 0.999557614326477),\n",
       " ('justify', 0.9995567798614502)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyed_vectors.most_similar(positive=['authority','destruction'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_sub_concepts = ['abide', 'allegiance', 'authority','class','command', 'compliant', 'control', 'defer', 'father', 'hierarchy', 'duty', 'honor', 'law', 'leader', 'agitate', 'alienate', 'defector', 'defiant', 'defy', 'denounce', 'disobey', 'disrespect', 'dissent', 'dissident', 'illegal', 'insubordinate', 'insurgent', 'obstruct'] \n",
    "auth_concepts = [concept for concept in auth_sub_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('put', 0.9997683763504028),\n",
       " ('getting', 0.9997579455375671),\n",
       " ('mean', 0.9997470378875732),\n",
       " ('new', 0.9997463226318359),\n",
       " ('hell', 0.9997445344924927),\n",
       " ('could', 0.999735951423645),\n",
       " ('yes', 0.9997355937957764),\n",
       " ('since', 0.9997342824935913),\n",
       " ('clearly', 0.9997342228889465),\n",
       " ('workers', 0.9997329115867615),\n",
       " ('law', 0.9997326135635376),\n",
       " ('use', 0.999731719493866),\n",
       " ('certain', 0.9997316002845764),\n",
       " ('must', 0.9997310042381287),\n",
       " ('comment', 0.9997291564941406)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyed_vectors.most_similar(positive=['purity','shame'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_degrad_concepts = ['austerity', 'celibate', 'chaste','church','clean', 'decent', 'holy', 'immaculate', 'innocent', 'modest', 'pious', 'pristine', 'pure', 'sacred', 'adultery', 'blemish', 'contagious', 'debase', 'debauchery', 'defile', 'desecrate', 'dirt', 'disease', 'disgust', 'exploitation', 'filth', 'gross', 'impiety'] \n",
    "san_concepts = [concept for concept in san_degrad_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('based', 0.9990540742874146),\n",
       " ('media', 0.9990530610084534),\n",
       " ('white', 0.9990503191947937),\n",
       " ('clearly', 0.9990448355674744),\n",
       " ('high', 0.9990434646606445),\n",
       " ('policy', 0.9990425109863281),\n",
       " ('laws', 0.9990400075912476),\n",
       " ('workers', 0.9990375638008118),\n",
       " ('also', 0.9990370273590088),\n",
       " ('best', 0.9990361928939819),\n",
       " ('country', 0.9990361332893372),\n",
       " ('public', 0.9990347027778625),\n",
       " ('completely', 0.9990332126617432),\n",
       " ('values', 0.9990312457084656),\n",
       " ('oh', 0.9990310072898865)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyed_vectors.most_similar(positive=['liberty','oppression'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_opp_concepts = ['blameless', 'canon', 'character','commendable','correct', 'decent', 'doctrine', 'ethics', 'exemplary', 'good', 'goodness', 'honest', 'legal', 'integrity', 'bad', 'evil', 'immoral', 'indecent', 'offend', 'offensive', 'transgress', 'wicked', 'wretched', 'wrong'] \n",
    "lib_concepts = [concept for concept in lib_opp_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Semantic Sentiment Scores by OSSA model\n",
    "overall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n",
    "                                                   care_target_tokens= care_concepts, \n",
    "                                                   fair_target_tokens= fair_concepts,\n",
    "                                                   loyal_target_tokens= loyal_concepts,\n",
    "                                                   auth_target_tokens= auth_concepts,\n",
    "                                                   san_target_tokens= san_concepts,\n",
    "                                                   lib_target_tokens= lib_concepts,\n",
    "                                                   doc_tokens = df['tokenized_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store semantic sentiment store computed by OSSA model in df\n",
    "df['overall_care'] = overall_df_scores[0] \n",
    "df['overall_fair'] = overall_df_scores[1] \n",
    "df['overall_loyal'] = overall_df_scores[2]\n",
    "df['overall_auth'] = overall_df_scores[3]\n",
    "df['overall_san'] = overall_df_scores[4]\n",
    "df['overall_lib'] = overall_df_scores[5]\n",
    "df['overall_max_score'] = overall_df_scores[6]\n",
    "df['moral_foundations'] = overall_df_scores[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>labeled_data</th>\n",
       "      <th>tokenized_vectors</th>\n",
       "      <th>overall_care</th>\n",
       "      <th>overall_fair</th>\n",
       "      <th>overall_loyal</th>\n",
       "      <th>overall_auth</th>\n",
       "      <th>overall_san</th>\n",
       "      <th>overall_lib</th>\n",
       "      <th>overall_max_score</th>\n",
       "      <th>moral_foundations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "      <td>4</td>\n",
       "      <td>[particular, part, debate, especially, funny, ...</td>\n",
       "      <td>0.753502</td>\n",
       "      <td>0.840363</td>\n",
       "      <td>0.861728</td>\n",
       "      <td>0.790605</td>\n",
       "      <td>0.876608</td>\n",
       "      <td>0.860197</td>\n",
       "      <td>0.876608</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>TBH Marion Le Pen would be better. Closet fasc...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>tbh marion le pen well closet fascist vs flamb...</td>\n",
       "      <td>1</td>\n",
       "      <td>[tbh, marion, le, pen, would, better, closet, ...</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.534338</td>\n",
       "      <td>0.638900</td>\n",
       "      <td>0.543265</td>\n",
       "      <td>0.623580</td>\n",
       "      <td>0.606955</td>\n",
       "      <td>0.638900</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>The Le Pen brand of conservatism and classical...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>le pen brand conservatism classical right wing...</td>\n",
       "      <td>3</td>\n",
       "      <td>[le, pen, brand, conservatism, classical, righ...</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>0.798543</td>\n",
       "      <td>0.837738</td>\n",
       "      <td>0.767958</td>\n",
       "      <td>0.807040</td>\n",
       "      <td>0.754511</td>\n",
       "      <td>0.837738</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
       "      <td>Loyalty,Equality</td>\n",
       "      <td>hey fuck leftist support le pen especially con...</td>\n",
       "      <td>5</td>\n",
       "      <td>[hey, fuck, us, leftists, never, support, le, ...</td>\n",
       "      <td>0.719094</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>0.727889</td>\n",
       "      <td>0.819091</td>\n",
       "      <td>0.766099</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>hey fuck leftist support le pen especially con...</td>\n",
       "      <td>4</td>\n",
       "      <td>[hey, fuck, us, leftists, never, support, le, ...</td>\n",
       "      <td>0.719094</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>0.727889</td>\n",
       "      <td>0.819091</td>\n",
       "      <td>0.766099</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>hey fuck leftist support le pen especially con...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hey, fuck, us, leftists, never, support, le, ...</td>\n",
       "      <td>0.719094</td>\n",
       "      <td>0.725482</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>0.727889</td>\n",
       "      <td>0.819091</td>\n",
       "      <td>0.766099</td>\n",
       "      <td>0.828602</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Clearly there were enough to affect the result...</td>\n",
       "      <td>Equality</td>\n",
       "      <td>clearly affect result election mid west thankf...</td>\n",
       "      <td>1</td>\n",
       "      <td>[clearly, enough, affect, results, election, m...</td>\n",
       "      <td>0.751401</td>\n",
       "      <td>0.818181</td>\n",
       "      <td>0.867205</td>\n",
       "      <td>0.836998</td>\n",
       "      <td>0.857448</td>\n",
       "      <td>0.773556</td>\n",
       "      <td>0.867205</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>You are simplifying it. Islam is not the sole ...</td>\n",
       "      <td>Care</td>\n",
       "      <td>simplify islam sole reason terrorism western c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[simplifying, islam, NOTthe, sole, reason, beh...</td>\n",
       "      <td>0.761447</td>\n",
       "      <td>0.829347</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>0.838433</td>\n",
       "      <td>0.879134</td>\n",
       "      <td>0.779269</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>You are simplifying it. Islam is not the sole ...</td>\n",
       "      <td>Care,Loyalty</td>\n",
       "      <td>simplify islam sole reason terrorism western c...</td>\n",
       "      <td>5</td>\n",
       "      <td>[simplifying, islam, NOTthe, sole, reason, beh...</td>\n",
       "      <td>0.761447</td>\n",
       "      <td>0.829347</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>0.838433</td>\n",
       "      <td>0.879134</td>\n",
       "      <td>0.779269</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>You are simplifying it. Islam is not the sole ...</td>\n",
       "      <td>Care</td>\n",
       "      <td>simplify islam sole reason terrorism western c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[simplifying, islam, NOTthe, sole, reason, beh...</td>\n",
       "      <td>0.761447</td>\n",
       "      <td>0.829347</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>0.838433</td>\n",
       "      <td>0.879134</td>\n",
       "      <td>0.779269</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Wow did not know all that! Maybe got some sour...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>wow maybe get source read consider honestly pr...</td>\n",
       "      <td>3</td>\n",
       "      <td>[wow, NOTknow, maybe, got, sources, read, thou...</td>\n",
       "      <td>0.669792</td>\n",
       "      <td>0.719547</td>\n",
       "      <td>0.772858</td>\n",
       "      <td>0.698385</td>\n",
       "      <td>0.774867</td>\n",
       "      <td>0.768700</td>\n",
       "      <td>0.774867</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "      <td>[valls, disgusting, traitor, party, helped, ma...</td>\n",
       "      <td>0.518808</td>\n",
       "      <td>0.654490</td>\n",
       "      <td>0.671165</td>\n",
       "      <td>0.646418</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>0.686118</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Purity,Loyalty,Authority</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>5</td>\n",
       "      <td>[valls, disgusting, traitor, party, helped, ma...</td>\n",
       "      <td>0.518808</td>\n",
       "      <td>0.654490</td>\n",
       "      <td>0.671165</td>\n",
       "      <td>0.646418</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>0.686118</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "      <td>[valls, disgusting, traitor, party, helped, ma...</td>\n",
       "      <td>0.518808</td>\n",
       "      <td>0.654490</td>\n",
       "      <td>0.671165</td>\n",
       "      <td>0.646418</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>0.686118</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Melenchon's party said they'll hold an interna...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>melenchon party say hold internal referendum d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[melenchon, party, said, hold, internal, refer...</td>\n",
       "      <td>0.551530</td>\n",
       "      <td>0.657034</td>\n",
       "      <td>0.707890</td>\n",
       "      <td>0.678820</td>\n",
       "      <td>0.708063</td>\n",
       "      <td>0.656058</td>\n",
       "      <td>0.708063</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Melenchon's party said they'll hold an interna...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>melenchon party say hold internal referendum d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[melenchon, party, said, hold, internal, refer...</td>\n",
       "      <td>0.551530</td>\n",
       "      <td>0.657034</td>\n",
       "      <td>0.707890</td>\n",
       "      <td>0.678820</td>\n",
       "      <td>0.708063</td>\n",
       "      <td>0.656058</td>\n",
       "      <td>0.708063</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>I believe that Le Pen's desire to reduce Musli...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>believe le pen desire reduce muslim immigratio...</td>\n",
       "      <td>3</td>\n",
       "      <td>[believe, le, pen, desire, reduce, muslim, imm...</td>\n",
       "      <td>0.614740</td>\n",
       "      <td>0.700399</td>\n",
       "      <td>0.785886</td>\n",
       "      <td>0.719027</td>\n",
       "      <td>0.745921</td>\n",
       "      <td>0.670959</td>\n",
       "      <td>0.785886</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>Melenchon is much more supportive of instituti...</td>\n",
       "      <td>Purity,Loyalty</td>\n",
       "      <td>melenchon supportive institution le pen evil s...</td>\n",
       "      <td>5</td>\n",
       "      <td>[melenchon, much, supportive, institutions, le...</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>0.616834</td>\n",
       "      <td>0.704314</td>\n",
       "      <td>0.617468</td>\n",
       "      <td>0.690145</td>\n",
       "      <td>0.632185</td>\n",
       "      <td>0.704314</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>Everything you're listing as problems with neo...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>list problem neoliberalism consequence neolibe...</td>\n",
       "      <td>3</td>\n",
       "      <td>[everything, listing, problems, neoliberalism,...</td>\n",
       "      <td>0.740107</td>\n",
       "      <td>0.833024</td>\n",
       "      <td>0.870072</td>\n",
       "      <td>0.820914</td>\n",
       "      <td>0.868435</td>\n",
       "      <td>0.824712</td>\n",
       "      <td>0.870072</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>Everything you're listing as problems with neo...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>list problem neoliberalism consequence neolibe...</td>\n",
       "      <td>3</td>\n",
       "      <td>[everything, listing, problems, neoliberalism,...</td>\n",
       "      <td>0.740107</td>\n",
       "      <td>0.833024</td>\n",
       "      <td>0.870072</td>\n",
       "      <td>0.820914</td>\n",
       "      <td>0.868435</td>\n",
       "      <td>0.824712</td>\n",
       "      <td>0.870072</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text  \\\n",
       "1            1  That particular part of the debate is especial...   \n",
       "8            8  TBH Marion Le Pen would be better. Closet fasc...   \n",
       "12          12  The Le Pen brand of conservatism and classical...   \n",
       "21          21  Hey, fuck you. Us leftists will never support ...   \n",
       "22          22  Hey, fuck you. Us leftists will never support ...   \n",
       "23          23  Hey, fuck you. Us leftists will never support ...   \n",
       "24          24  Clearly there were enough to affect the result...   \n",
       "27          27  You are simplifying it. Islam is not the sole ...   \n",
       "28          28  You are simplifying it. Islam is not the sole ...   \n",
       "29          29  You are simplifying it. Islam is not the sole ...   \n",
       "32          32  Wow did not know all that! Maybe got some sour...   \n",
       "42          42  &gt; Valls is such a disgusting traitor to his...   \n",
       "43          43  &gt; Valls is such a disgusting traitor to his...   \n",
       "44          44  &gt; Valls is such a disgusting traitor to his...   \n",
       "46          46  Melenchon's party said they'll hold an interna...   \n",
       "47          47  Melenchon's party said they'll hold an interna...   \n",
       "50          50  I believe that Le Pen's desire to reduce Musli...   \n",
       "52          52  Melenchon is much more supportive of instituti...   \n",
       "54          54  Everything you're listing as problems with neo...   \n",
       "56          56  Everything you're listing as problems with neo...   \n",
       "\n",
       "                  annotation  \\\n",
       "1                     Purity   \n",
       "8                   Equality   \n",
       "12                 Authority   \n",
       "21          Loyalty,Equality   \n",
       "22                    Purity   \n",
       "23                  Equality   \n",
       "24                  Equality   \n",
       "27                      Care   \n",
       "28              Care,Loyalty   \n",
       "29                      Care   \n",
       "32                 Authority   \n",
       "42                   Loyalty   \n",
       "43  Purity,Loyalty,Authority   \n",
       "44                   Loyalty   \n",
       "46                   Loyalty   \n",
       "47                   Loyalty   \n",
       "50                 Authority   \n",
       "52            Purity,Loyalty   \n",
       "54                 Authority   \n",
       "56                 Authority   \n",
       "\n",
       "                                           lemmatized  labeled_data  \\\n",
       "1   particular debate especially funny macron expl...             4   \n",
       "8   tbh marion le pen well closet fascist vs flamb...             1   \n",
       "12  le pen brand conservatism classical right wing...             3   \n",
       "21  hey fuck leftist support le pen especially con...             5   \n",
       "22  hey fuck leftist support le pen especially con...             4   \n",
       "23  hey fuck leftist support le pen especially con...             1   \n",
       "24  clearly affect result election mid west thankf...             1   \n",
       "27  simplify islam sole reason terrorism western c...             0   \n",
       "28  simplify islam sole reason terrorism western c...             5   \n",
       "29  simplify islam sole reason terrorism western c...             0   \n",
       "32  wow maybe get source read consider honestly pr...             3   \n",
       "42  valls disgusting traitor party help macron win...             2   \n",
       "43  valls disgusting traitor party help macron win...             5   \n",
       "44  valls disgusting traitor party help macron win...             2   \n",
       "46  melenchon party say hold internal referendum d...             2   \n",
       "47  melenchon party say hold internal referendum d...             2   \n",
       "50  believe le pen desire reduce muslim immigratio...             3   \n",
       "52  melenchon supportive institution le pen evil s...             5   \n",
       "54  list problem neoliberalism consequence neolibe...             3   \n",
       "56  list problem neoliberalism consequence neolibe...             3   \n",
       "\n",
       "                                    tokenized_vectors  overall_care  \\\n",
       "1   [particular, part, debate, especially, funny, ...      0.753502   \n",
       "8   [tbh, marion, le, pen, would, better, closet, ...      0.463816   \n",
       "12  [le, pen, brand, conservatism, classical, righ...      0.655129   \n",
       "21  [hey, fuck, us, leftists, never, support, le, ...      0.719094   \n",
       "22  [hey, fuck, us, leftists, never, support, le, ...      0.719094   \n",
       "23  [hey, fuck, us, leftists, never, support, le, ...      0.719094   \n",
       "24  [clearly, enough, affect, results, election, m...      0.751401   \n",
       "27  [simplifying, islam, NOTthe, sole, reason, beh...      0.761447   \n",
       "28  [simplifying, islam, NOTthe, sole, reason, beh...      0.761447   \n",
       "29  [simplifying, islam, NOTthe, sole, reason, beh...      0.761447   \n",
       "32  [wow, NOTknow, maybe, got, sources, read, thou...      0.669792   \n",
       "42  [valls, disgusting, traitor, party, helped, ma...      0.518808   \n",
       "43  [valls, disgusting, traitor, party, helped, ma...      0.518808   \n",
       "44  [valls, disgusting, traitor, party, helped, ma...      0.518808   \n",
       "46  [melenchon, party, said, hold, internal, refer...      0.551530   \n",
       "47  [melenchon, party, said, hold, internal, refer...      0.551530   \n",
       "50  [believe, le, pen, desire, reduce, muslim, imm...      0.614740   \n",
       "52  [melenchon, much, supportive, institutions, le...      0.545608   \n",
       "54  [everything, listing, problems, neoliberalism,...      0.740107   \n",
       "56  [everything, listing, problems, neoliberalism,...      0.740107   \n",
       "\n",
       "    overall_fair  overall_loyal  overall_auth  overall_san  overall_lib  \\\n",
       "1       0.840363       0.861728      0.790605     0.876608     0.860197   \n",
       "8       0.534338       0.638900      0.543265     0.623580     0.606955   \n",
       "12      0.798543       0.837738      0.767958     0.807040     0.754511   \n",
       "21      0.725482       0.828602      0.727889     0.819091     0.766099   \n",
       "22      0.725482       0.828602      0.727889     0.819091     0.766099   \n",
       "23      0.725482       0.828602      0.727889     0.819091     0.766099   \n",
       "24      0.818181       0.867205      0.836998     0.857448     0.773556   \n",
       "27      0.829347       0.903385      0.838433     0.879134     0.779269   \n",
       "28      0.829347       0.903385      0.838433     0.879134     0.779269   \n",
       "29      0.829347       0.903385      0.838433     0.879134     0.779269   \n",
       "32      0.719547       0.772858      0.698385     0.774867     0.768700   \n",
       "42      0.654490       0.671165      0.646418     0.694994     0.686118   \n",
       "43      0.654490       0.671165      0.646418     0.694994     0.686118   \n",
       "44      0.654490       0.671165      0.646418     0.694994     0.686118   \n",
       "46      0.657034       0.707890      0.678820     0.708063     0.656058   \n",
       "47      0.657034       0.707890      0.678820     0.708063     0.656058   \n",
       "50      0.700399       0.785886      0.719027     0.745921     0.670959   \n",
       "52      0.616834       0.704314      0.617468     0.690145     0.632185   \n",
       "54      0.833024       0.870072      0.820914     0.868435     0.824712   \n",
       "56      0.833024       0.870072      0.820914     0.868435     0.824712   \n",
       "\n",
       "    overall_max_score  moral_foundations  \n",
       "1            0.876608                2.0  \n",
       "8            0.638900                2.0  \n",
       "12           0.837738                4.0  \n",
       "21           0.828602                1.0  \n",
       "22           0.828602                2.0  \n",
       "23           0.828602                2.0  \n",
       "24           0.867205                2.0  \n",
       "27           0.903385                5.0  \n",
       "28           0.903385                3.0  \n",
       "29           0.903385                3.0  \n",
       "32           0.774867                3.0  \n",
       "42           0.694994                5.0  \n",
       "43           0.694994                2.0  \n",
       "44           0.694994                2.0  \n",
       "46           0.708063                2.0  \n",
       "47           0.708063                2.0  \n",
       "50           0.785886                2.0  \n",
       "52           0.704314                2.0  \n",
       "54           0.870072                2.0  \n",
       "56           0.870072                2.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import w2v_utils\n",
    "importlib.reload(w2v_utils)\n",
    "from w2v_utils import(evaluate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2320, 2321, 2322, ..., 3411, 3412, 3413], dtype=int64),)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pd.isnull(df['moral_foundations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['moral_foundations'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSSA Model Evaluation: \n",
      "* Accuracy Score:  21.8966%\n",
      "* F1 Score:  21.8966%\n",
      "* Recall Score:  21.8966%\n",
      "* Precision Score:  21.8966%\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "# OSSA Model Evaluation\n",
    "print(\"OSSA Model Evaluation: \")\n",
    "evaluate_model(df['labeled_data'], \n",
    "               df['moral_foundations'])\n",
    "\n",
    "print(\"=======================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned Data with Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processing and Data manipulation\n",
    "import numpy as np # linear algenra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries and packages for NLP\n",
    "import nltk\n",
    "# It includes a set of text \n",
    "# processing libraries for classification, tokenization, \n",
    "# stemming, tagging, parsing, and semantic reasonin\n",
    "import gensim\n",
    "# library for unsupervised topic modeling, \n",
    "# document indexing, retrieval by similarity, and \n",
    "# other natural language processing functionalities, \n",
    "# using modern statistical machine learning.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"C:\\\\Users\\\\CACER\\\\OneDrive\\\\Desktop\\\\lemmatized_reddit_og.csv\")\n",
    "df_new.drop_duplicates(inplace= True)\n",
    "df_new.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>bucket</th>\n",
       "      <th>annotator</th>\n",
       "      <th>annotation</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator03</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>Confident</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator01</td>\n",
       "      <td>Purity</td>\n",
       "      <td>Confident</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator02</td>\n",
       "      <td>Thin Morality</td>\n",
       "      <td>Confident</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator03</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>Confident</td>\n",
       "      <td>france pretty lively lingo usually deliberatel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator00</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>Somewhat Confident</td>\n",
       "      <td>france pretty lively lingo usually deliberatel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text subreddit  \\\n",
       "0           0  That particular part of the debate is especial...    europe   \n",
       "1           1  That particular part of the debate is especial...    europe   \n",
       "2           2  That particular part of the debate is especial...    europe   \n",
       "3           3  /r/france is pretty lively, with it's own ling...    europe   \n",
       "4           4  /r/france is pretty lively, with it's own ling...    europe   \n",
       "\n",
       "            bucket    annotator     annotation          confidence  \\\n",
       "0  French politics  annotator03      Non-Moral           Confident   \n",
       "1  French politics  annotator01         Purity           Confident   \n",
       "2  French politics  annotator02  Thin Morality           Confident   \n",
       "3  French politics  annotator03      Non-Moral           Confident   \n",
       "4  French politics  annotator00      Non-Moral  Somewhat Confident   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  particular debate especially funny macron expl...  \n",
       "1  particular debate especially funny macron expl...  \n",
       "2  particular debate especially funny macron expl...  \n",
       "3  france pretty lively lingo usually deliberatel...  \n",
       "4  france pretty lively lingo usually deliberatel...  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=['subreddit', 'bucket', 'annotator', 'confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Thin Morality</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>france pretty lively lingo usually deliberatel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/r/france is pretty lively, with it's own ling...</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>france pretty lively lingo usually deliberatel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  That particular part of the debate is especial...   \n",
       "1           1  That particular part of the debate is especial...   \n",
       "2           2  That particular part of the debate is especial...   \n",
       "3           3  /r/france is pretty lively, with it's own ling...   \n",
       "4           4  /r/france is pretty lively, with it's own ling...   \n",
       "\n",
       "      annotation                                         lemmatized  \n",
       "0      Non-Moral  particular debate especially funny macron expl...  \n",
       "1         Purity  particular debate especially funny macron expl...  \n",
       "2  Thin Morality  particular debate especially funny macron expl...  \n",
       "3      Non-Moral  france pretty lively lingo usually deliberatel...  \n",
       "4      Non-Moral  france pretty lively lingo usually deliberatel...  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(df_new[df_new['annotation'] == 'Authority'].sample(frac=0.11).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Authority          863\n",
       "Loyalty            857\n",
       "Equality           854\n",
       "Care               854\n",
       "Purity             845\n",
       "Proportionality    829\n",
       "Name: annotation, dtype: int64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"annotation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping \n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality \") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Loyalty,Purity,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Loyalty,Purity,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Loyalty,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Loyalty,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Purity,Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Proportionality,Care,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity,Equality,Loyalty,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Proportionality,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Care,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Purity,Equality,Authority,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Care,Equality \") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Care,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Purity,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Loyalty,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Loyalty,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Equality,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Authority,Equality,Proportionality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Loyalty,Equality,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Purity,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Proportionality,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Care\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Care,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Care,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Equality,Purity\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Care,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Equality,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Loyalty,Authority,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Purity,Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Non-Moral\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Thin Morality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Non-Moral\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Thin Morality,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Care,Thin Morality,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Authority,Loyalty\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Purity,Loyalty,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Purity,Proportionality,Equality\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Equality,Loyalty,Authority\") == False]\n",
    "df_new = df_new[df_new[\"annotation\"].str.contains(\"Proportionality,Care,Loyalty,Authority\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_values(df, input_column, output_column):\n",
    "    df[output_column] = df[input_column].apply(lambda x: 0 if (x == 'Care') else 1 if (x == 'Equality') else 2 if (x == 'Loyalty') else 3 if (x == 'Authority') else 4 if (x == 'Purity') else 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_values(df_new, \"annotation\", \"labeled_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>labeled_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>hey fuck leftist support le pen especially con...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Melenchon's party said they'll hold an interna...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>melenchon party say hold internal referendum d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text annotation  \\\n",
       "1            1  That particular part of the debate is especial...     Purity   \n",
       "22          22  Hey, fuck you. Us leftists will never support ...     Purity   \n",
       "42          42  &gt; Valls is such a disgusting traitor to his...    Loyalty   \n",
       "44          44  &gt; Valls is such a disgusting traitor to his...    Loyalty   \n",
       "46          46  Melenchon's party said they'll hold an interna...    Loyalty   \n",
       "\n",
       "                                           lemmatized  labeled_data  \n",
       "1   particular debate especially funny macron expl...             4  \n",
       "22  hey fuck leftist support le pen especially con...             4  \n",
       "42  valls disgusting traitor party help macron win...             2  \n",
       "44  valls disgusting traitor party help macron win...             2  \n",
       "46  melenchon party say hold internal referendum d...             2  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding `src` directory to the directories for interpreter to search\n",
    "sys.path.append(os.path.abspath(os.path.join('../..','w2v_utils.py')))\n",
    "\n",
    "\n",
    "# Importing functions and classes from utility module\n",
    "from w2v_utils import (Tokenizer,\n",
    "                       w2v_trainer,\n",
    "                       calculate_overall_similarity_score,\n",
    "                       overall_semantic_sentiment_analysis, \n",
    "                       evaluate_model\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancing the Tokenizer class\n",
    "tokenizer = Tokenizer(clean= True,\n",
    "                      lower= True, \n",
    "                      de_noise= True, \n",
    "                      remove_stop_words= True,\n",
    "                      keep_negation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews\n",
    "df_new['tokenized_vectors'] = df_new['lemmatized'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Word2Vec model\n",
    "keyed_vectors, keyed_vocab = w2v_trainer(df_new['tokenized_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('set', 0.9997982382774353),\n",
       " ('test', 0.999783456325531),\n",
       " ('normal', 0.9997734427452087),\n",
       " ('ass', 0.9997714161872864),\n",
       " ('fire', 0.9997702836990356),\n",
       " ('lie', 0.9997661113739014),\n",
       " ('lack', 0.9997650384902954),\n",
       " ('employee', 0.9997637271881104),\n",
       " ('protect', 0.9997631907463074),\n",
       " ('responsibility', 0.999762237071991),\n",
       " ('son', 0.9997559785842896),\n",
       " ('possible', 0.9997543692588806),\n",
       " ('house', 0.9997540712356567),\n",
       " ('absolutely', 0.9997536540031433),\n",
       " ('healthy', 0.9997531771659851)]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"care/harm\" \n",
    "keyed_vectors.most_similar(positive=['care','harm'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_harm_concepts = ['care', 'benefit', 'amity','caring','compassion', 'empath', 'guard', 'peace', 'protect', 'safe', 'secure', 'shelter', 'shield', 'sympathy', 'abuse', 'annihilate', 'attack', 'brutal', 'cruelty', 'crush', 'damage', 'destroy', 'detriment', 'endanger', 'fight', 'harm', 'hurt', 'kill'] \n",
    "care_concepts = [concept for concept in care_harm_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demand', 0.999862551689148),\n",
       " ('bit', 0.999858021736145),\n",
       " ('authority', 0.999854564666748),\n",
       " ('level', 0.9998540878295898),\n",
       " ('sound', 0.9998457431793213),\n",
       " ('ability', 0.9998427033424377),\n",
       " ('idea', 0.9998414516448975),\n",
       " ('lack', 0.999841034412384),\n",
       " ('action', 0.9998390078544617),\n",
       " ('accept', 0.9998366832733154),\n",
       " ('daily', 0.9998350143432617),\n",
       " ('kind', 0.9998345375061035),\n",
       " ('write', 0.9998315572738647),\n",
       " ('learn', 0.9998311996459961),\n",
       " ('possible', 0.9998305439949036)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"Fairness/cheating\" \n",
    "keyed_vectors.most_similar(positive=['fair','avoid'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that all 'fairness_cheating_concepts' are in the keyed word2vec vovabulary\n",
    "# Here we are added more words that are associated into the positive words vector\n",
    "fair_cheat_concepts = ['level','equally','fair', 'balance', 'constant','egalitarian','equable', 'equal', 'equity', 'fairminded', 'honest', 'fairly', 'impartial', 'justice', 'tolerant', 'bias', 'bigotry', 'discrimination', 'dishonest', 'exclusion', 'favoritism', 'inequitable', 'injustice', 'preference', 'prejudice', 'segregation', 'unequal', 'unfair', 'unjust'] \n",
    "fair_concepts = [concept for concept in fair_cheat_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turn', 0.9998564124107361),\n",
       " ('history', 0.9998404383659363),\n",
       " ('r', 0.9998374581336975),\n",
       " ('major', 0.9998307228088379),\n",
       " ('remember', 0.9998297691345215),\n",
       " ('war', 0.9998262524604797),\n",
       " ('hold', 0.9998218417167664),\n",
       " ('blame', 0.9998170733451843),\n",
       " ('message', 0.999815821647644),\n",
       " ('ok', 0.999809980392456),\n",
       " ('course', 0.9998095035552979),\n",
       " ('citizen', 0.9998091459274292),\n",
       " ('bring', 0.9998080134391785),\n",
       " ('argue', 0.9998075366020203),\n",
       " ('share', 0.9998050332069397)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"loyalty/betrayal\" \n",
    "keyed_vectors.most_similar(positive=['loyalty','false'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that all 'loyal_betrayal_concepts' are in the keyed word2vec vovabulary\n",
    "# Here we are added more words that are associated into the positive words vector\n",
    "loyal_betrayal_concepts = ['ally', 'cadre', 'clique','cohort','collective', 'communal', 'community', 'comrade', 'devote', 'familial', 'families', 'family', 'fellow', 'group', 'deceive', 'enemy', 'foregin', 'immigrant', 'imposter', 'individual', 'jilt', 'miscreant', 'renegade', 'sequester', 'spy', 'terrorist'] \n",
    "loyal_concepts = [concept for concept in loyal_betrayal_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 0.9998120665550232),\n",
       " ('normal', 0.9998102784156799),\n",
       " ('employee', 0.9998093843460083),\n",
       " ('play', 0.999809205532074),\n",
       " ('lack', 0.9998088479042053),\n",
       " ('kind', 0.9998056888580322),\n",
       " ('mother', 0.9998041987419128),\n",
       " ('house', 0.9998036026954651),\n",
       " ('fire', 0.9998021125793457),\n",
       " ('service', 0.9998015761375427),\n",
       " ('write', 0.9997997283935547),\n",
       " ('dog', 0.9997979402542114),\n",
       " ('huge', 0.999797523021698),\n",
       " ('personal', 0.9997972249984741),\n",
       " ('level', 0.9997959733009338)]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"Authority/Subversion\" \n",
    "keyed_vectors.most_similar(positive=['authority','destruction'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that all 'authority_subversion_concepts' are in the keyed word2vec vovabulary\n",
    "# Here we are added more words that are associated into the positive words vector\n",
    "auth_sub_concepts = ['abide', 'allegiance', 'authority','class','command', 'compliant', 'control', 'defer', 'father', 'hierarchy', 'duty', 'honor', 'law', 'leader', 'agitate', 'alienate', 'defector', 'defiant', 'defy', 'denounce', 'disobey', 'disrespect', 'dissent', 'dissident', 'illegal', 'insubordinate', 'insurgent', 'obstruct'] \n",
    "auth_concepts = [concept for concept in auth_sub_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bit', 0.9998696446418762),\n",
       " ('figure', 0.9998604655265808),\n",
       " ('demand', 0.9998602271080017),\n",
       " ('authority', 0.9998573660850525),\n",
       " ('learn', 0.9998548030853271),\n",
       " ('level', 0.9998518824577332),\n",
       " ('concern', 0.9998475313186646),\n",
       " ('action', 0.9998473525047302),\n",
       " ('lie', 0.9998448491096497),\n",
       " ('honestly', 0.9998417496681213),\n",
       " ('lack', 0.9998387694358826),\n",
       " ('idea', 0.9998369216918945),\n",
       " ('expect', 0.9998364448547363),\n",
       " ('fire', 0.9998348355293274),\n",
       " ('assume', 0.9998340606689453)]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"sanctity/degradation\" \n",
    "keyed_vectors.most_similar(positive=['faith','shame'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that all 'sancity_degrad_concepts' are in the keyed word2vec vovabulary\n",
    "# Here we are added more words that are associated into the positive words vector\n",
    "san_degrad_concepts = ['austerity', 'celibate', 'chaste','church','clean', 'decent', 'holy', 'immaculate', 'innocent', 'modest', 'pious', 'pristine', 'pure', 'sacred', 'adultery', 'blemish', 'contagious', 'debase', 'debauchery', 'defile', 'desecrate', 'dirt', 'disease', 'disgust', 'exploitation', 'filth', 'gross', 'impiety'] \n",
    "san_concepts = [concept for concept in san_degrad_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('increase', 0.999697744846344),\n",
       " ('e', 0.9996851086616516),\n",
       " ('compare', 0.999667763710022),\n",
       " ('bunch', 0.9996671080589294),\n",
       " ('effect', 0.9996620416641235),\n",
       " ('oh', 0.9996575713157654),\n",
       " ('dangerous', 0.9996569752693176),\n",
       " ('include', 0.9996509552001953),\n",
       " ('charge', 0.9996486306190491),\n",
       " ('false', 0.9996448755264282),\n",
       " ('prove', 0.9996404051780701),\n",
       " ('bring', 0.9996398687362671),\n",
       " ('fight', 0.999635636806488),\n",
       " ('ban', 0.9996339678764343),\n",
       " ('stop', 0.9996328353881836)]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"liberty/oppression\" \n",
    "keyed_vectors.most_similar(positive=['liberty','oppression'], negative=[], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that all 'liberty_oppression_concepts' are in the keyed word2vec vovabulary\n",
    "# Here we are added more words that are associated into the positive words vector\n",
    "lib_opp_concepts = ['blameless', 'canon', 'character','commendable','correct', 'decent', 'doctrine', 'ethics', 'exemplary', 'good', 'goodness', 'honest', 'legal', 'integrity', 'bad', 'evil', 'immoral', 'indecent', 'offend', 'offensive', 'transgress', 'wicked', 'wretched', 'wrong'] \n",
    "lib_concepts = [concept for concept in lib_opp_concepts if concept in keyed_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import w2v_utils\n",
    "importlib.reload(w2v_utils)\n",
    "from w2v_utils import(calculate_overall_similarity_score,\n",
    "                       overall_semantic_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Semantic Sentiment Scores by OSSA model\n",
    "overall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n",
    "                                                   care_target_tokens= care_concepts, \n",
    "                                                   fair_target_tokens= fair_concepts,\n",
    "                                                   loyal_target_tokens= loyal_concepts,\n",
    "                                                   auth_target_tokens= auth_concepts,\n",
    "                                                   san_target_tokens= san_concepts,\n",
    "                                                   lib_target_tokens= lib_concepts,\n",
    "                                                   doc_tokens = df_new['tokenized_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store semantic sentiment store computed by OSSA model in df\n",
    "df_new['overall_care'] = overall_df_scores[0] \n",
    "df_new['overall_fair'] = overall_df_scores[1] \n",
    "df_new['overall_loyal'] = overall_df_scores[2]\n",
    "df_new['overall_auth'] = overall_df_scores[3]\n",
    "df_new['overall_san'] = overall_df_scores[4]\n",
    "df_new['overall_lib'] = overall_df_scores[5]\n",
    "df_new['overall_max_score'] = overall_df_scores[6]\n",
    "df_new['moral_foundations'] = overall_df_scores[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>labeled_data</th>\n",
       "      <th>tokenized_vectors</th>\n",
       "      <th>overall_care</th>\n",
       "      <th>overall_fair</th>\n",
       "      <th>overall_loyal</th>\n",
       "      <th>overall_auth</th>\n",
       "      <th>overall_san</th>\n",
       "      <th>overall_lib</th>\n",
       "      <th>overall_max_score</th>\n",
       "      <th>moral_foundations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>particular debate especially funny macron expl...</td>\n",
       "      <td>4</td>\n",
       "      <td>[particular, debate, especially, funny, macron...</td>\n",
       "      <td>0.997654</td>\n",
       "      <td>0.997649</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998503</td>\n",
       "      <td>0.998323</td>\n",
       "      <td>0.998392</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
       "      <td>Purity</td>\n",
       "      <td>hey fuck leftist support le pen especially con...</td>\n",
       "      <td>4</td>\n",
       "      <td>[hey, fuck, leftist, support, le, pen, especia...</td>\n",
       "      <td>0.950842</td>\n",
       "      <td>0.950798</td>\n",
       "      <td>0.956711</td>\n",
       "      <td>0.955011</td>\n",
       "      <td>0.954246</td>\n",
       "      <td>0.954705</td>\n",
       "      <td>0.956711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "      <td>[valls, disgusting, traitor, party, help, macr...</td>\n",
       "      <td>0.989156</td>\n",
       "      <td>0.989078</td>\n",
       "      <td>0.991785</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>0.990676</td>\n",
       "      <td>0.990962</td>\n",
       "      <td>0.991785</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>&amp;gt; Valls is such a disgusting traitor to his...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>valls disgusting traitor party help macron win...</td>\n",
       "      <td>2</td>\n",
       "      <td>[valls, disgusting, traitor, party, help, macr...</td>\n",
       "      <td>0.989156</td>\n",
       "      <td>0.989078</td>\n",
       "      <td>0.991785</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>0.990676</td>\n",
       "      <td>0.990962</td>\n",
       "      <td>0.991785</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Melenchon's party said they'll hold an interna...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>melenchon party say hold internal referendum d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[melenchon, party, say, hold, internal, refere...</td>\n",
       "      <td>0.951920</td>\n",
       "      <td>0.951882</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>0.956089</td>\n",
       "      <td>0.955262</td>\n",
       "      <td>0.955718</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Melenchon's party said they'll hold an interna...</td>\n",
       "      <td>Loyalty</td>\n",
       "      <td>melenchon party say hold internal referendum d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[melenchon, party, say, hold, internal, refere...</td>\n",
       "      <td>0.951920</td>\n",
       "      <td>0.951882</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>0.956089</td>\n",
       "      <td>0.955262</td>\n",
       "      <td>0.955718</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>I believe that Le Pen's desire to reduce Musli...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>believe le pen desire reduce muslim immigratio...</td>\n",
       "      <td>3</td>\n",
       "      <td>[believe, le, pen, desire, reduce, muslim, imm...</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>0.925587</td>\n",
       "      <td>0.932880</td>\n",
       "      <td>0.930761</td>\n",
       "      <td>0.929776</td>\n",
       "      <td>0.930283</td>\n",
       "      <td>0.932880</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>Everything you're listing as problems with neo...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>list problem neoliberalism consequence neolibe...</td>\n",
       "      <td>3</td>\n",
       "      <td>[list, problem, neoliberalism, consequence, ne...</td>\n",
       "      <td>0.984132</td>\n",
       "      <td>0.984133</td>\n",
       "      <td>0.987451</td>\n",
       "      <td>0.986521</td>\n",
       "      <td>0.986036</td>\n",
       "      <td>0.986269</td>\n",
       "      <td>0.987451</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>I haven't been following the french election a...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>follow french election sure reason vote candid...</td>\n",
       "      <td>3</td>\n",
       "      <td>[follow, french, election, sure, reason, vote,...</td>\n",
       "      <td>0.967318</td>\n",
       "      <td>0.967280</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.970740</td>\n",
       "      <td>0.970072</td>\n",
       "      <td>0.970461</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>But it's not because of her euroscepticism tha...</td>\n",
       "      <td>Authority</td>\n",
       "      <td>euroscepticism vote le pen likely hate love po...</td>\n",
       "      <td>3</td>\n",
       "      <td>[euroscepticism, vote, le, pen, likely, hate, ...</td>\n",
       "      <td>0.938337</td>\n",
       "      <td>0.938297</td>\n",
       "      <td>0.944930</td>\n",
       "      <td>0.943024</td>\n",
       "      <td>0.942144</td>\n",
       "      <td>0.942648</td>\n",
       "      <td>0.944930</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text annotation  \\\n",
       "1            1  That particular part of the debate is especial...     Purity   \n",
       "22          22  Hey, fuck you. Us leftists will never support ...     Purity   \n",
       "42          42  &gt; Valls is such a disgusting traitor to his...    Loyalty   \n",
       "44          44  &gt; Valls is such a disgusting traitor to his...    Loyalty   \n",
       "46          46  Melenchon's party said they'll hold an interna...    Loyalty   \n",
       "47          47  Melenchon's party said they'll hold an interna...    Loyalty   \n",
       "50          50  I believe that Le Pen's desire to reduce Musli...  Authority   \n",
       "54          54  Everything you're listing as problems with neo...  Authority   \n",
       "76          76  I haven't been following the french election a...  Authority   \n",
       "83          83  But it's not because of her euroscepticism tha...  Authority   \n",
       "\n",
       "                                           lemmatized  labeled_data  \\\n",
       "1   particular debate especially funny macron expl...             4   \n",
       "22  hey fuck leftist support le pen especially con...             4   \n",
       "42  valls disgusting traitor party help macron win...             2   \n",
       "44  valls disgusting traitor party help macron win...             2   \n",
       "46  melenchon party say hold internal referendum d...             2   \n",
       "47  melenchon party say hold internal referendum d...             2   \n",
       "50  believe le pen desire reduce muslim immigratio...             3   \n",
       "54  list problem neoliberalism consequence neolibe...             3   \n",
       "76  follow french election sure reason vote candid...             3   \n",
       "83  euroscepticism vote le pen likely hate love po...             3   \n",
       "\n",
       "                                    tokenized_vectors  overall_care  \\\n",
       "1   [particular, debate, especially, funny, macron...      0.997654   \n",
       "22  [hey, fuck, leftist, support, le, pen, especia...      0.950842   \n",
       "42  [valls, disgusting, traitor, party, help, macr...      0.989156   \n",
       "44  [valls, disgusting, traitor, party, help, macr...      0.989156   \n",
       "46  [melenchon, party, say, hold, internal, refere...      0.951920   \n",
       "47  [melenchon, party, say, hold, internal, refere...      0.951920   \n",
       "50  [believe, le, pen, desire, reduce, muslim, imm...      0.925600   \n",
       "54  [list, problem, neoliberalism, consequence, ne...      0.984132   \n",
       "76  [follow, french, election, sure, reason, vote,...      0.967318   \n",
       "83  [euroscepticism, vote, le, pen, likely, hate, ...      0.938337   \n",
       "\n",
       "    overall_fair  overall_loyal  overall_auth  overall_san  overall_lib  \\\n",
       "1       0.997649       0.998800      0.998503     0.998323     0.998392   \n",
       "22      0.950798       0.956711      0.955011     0.954246     0.954705   \n",
       "42      0.989078       0.991785      0.991054     0.990676     0.990962   \n",
       "44      0.989078       0.991785      0.991054     0.990676     0.990962   \n",
       "46      0.951882       0.957774      0.956089     0.955262     0.955718   \n",
       "47      0.951882       0.957774      0.956089     0.955262     0.955718   \n",
       "50      0.925587       0.932880      0.930761     0.929776     0.930283   \n",
       "54      0.984133       0.987451      0.986521     0.986036     0.986269   \n",
       "76      0.967280       0.972112      0.970740     0.970072     0.970461   \n",
       "83      0.938297       0.944930      0.943024     0.942144     0.942648   \n",
       "\n",
       "    overall_max_score  moral_foundations  \n",
       "1            0.998800                  2  \n",
       "22           0.956711                  2  \n",
       "42           0.991785                  2  \n",
       "44           0.991785                  2  \n",
       "46           0.957774                  2  \n",
       "47           0.957774                  2  \n",
       "50           0.932880                  2  \n",
       "54           0.987451                  2  \n",
       "76           0.972112                  2  \n",
       "83           0.944930                  2  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new[df_new['moral_foundations'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSSA Model Evaluation: \n",
      "* Accuracy Score:  17.3127%\n",
      "* F1 Score:  17.3127%\n",
      "* Recall Score:  17.3127%\n",
      "* Precision Score:  17.3127%\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "# OSSA Model Evaluation\n",
    "print(\"OSSA Model Evaluation: \")\n",
    "evaluate_model(df_new['labeled_data'], \n",
    "               df_new['moral_foundations'])\n",
    "\n",
    "print(\"=======================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
